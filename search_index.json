[
["index.html", "rOpenSci Statistical Software Peer Review Chapter 1 Welcome", " rOpenSci Statistical Software Peer Review Mark Padgham and Noam Ross 2020-05-05 Chapter 1 Welcome Welcome to rOpenSci’s project developing software peer review system for statistical software in R packages and beyond. We invite you to join us in developing standards for peer-reviewed evaluation of statistical software. rOpenSci runs software peer review for R packages focused on data life cycle management. To apply these processes to software implementing statistical methods and algorithms, we need new ways of evaluating and testing software, and managing the review process. This book serves as a home for collecting research and developing those standards. As the book develops, it will become an extension of rOpenSci Packages: Development, Maintenance, and Peer Review, documenting not only our guidelines for statistical software but also the process of expanding the scope of review so it may be reproduced in other domains. You are invited to contribute to this project by filing suggestions as issues in the book’s GitHub repository. We also have a dedicated area for discussion on the rOpenSci forum. We have noted in the chapters we’re actively seeking more input and feedback by highlighting them their titles in red and adding [SEEKING FEEDBACK] to them. Feedback on other sections welcome, too - but they may be more or less developed. For this project we are lucky to have support of a great steering committee. The committee’s role is to help us with outreach to statistical communities, vetting community input and prioritizing our work. They are: Ben Bolker (@bolkerb) McMaster University Rebecca Killick, Lancaster University Stephanie Hicks (@stephaniehicks), Johns Hopkins University Max Kuhn (@topepos), RStudio Martin Morgan (@mt_morgan), Roswell Park Comprehensive Cancer Center This work is supported by the Sloan Foundation and is organized under an R Consortium Working Group. "],
["overview.html", "Chapter 2 Project Overview 2.1 Project Aims 2.2 Related projects and initiatives 2.3 Outline of this document 2.4 Community", " Chapter 2 Project Overview This chapter lays out key considerations, outstanding questions, and tasks for our first year of work, for purposes of generating community feedback for the project. It consists of the following sections: Scope of Statistical Software Review in which we address the scope of the project, and scopes of definition of “statistical software”. Standards for Statistical Software in which we consider the kinds of standards which might be developed and applied to assess statistical software. Software Assessment in which we provide a partial list of attributes and measures of software which might be usefully considered. Statistical Software Peer Review Process in which we consider questions regarding the possible forms and practices a peer review process might adopt. Each of these sections aims to highlight what we consider some of the most important questions and issues which the project will have to address. We generally do so by posing a list of “Proposals” at the end of each of the above-listed chapters. Each of these is intended to serve as a starting point for debate, and should be considered entirely open to discussion, modification, enhancement, removal, or any other suggestion which may arise. The Proposals are primarily intended in current form to provide focal points for further consideration and discussion prior to any concrete implementation. 2.1 Project Aims To foster a community of practice in which users and developers of statistical software mutually improve quality, reproducibility, and reliability of research. To provide software creators with a set of tools to assess the quality of their work, and a process by which to improve it. To provide statistical software developers, users, and consumers of results a discoverable “badge” that transparently conveys a level of assurance of software quality and may be usable as professional credit. To create a set of standards that may be adopted and adapted by open source and private groups, academic journals, or other statistical software evaluation projects. To focus on R as primary language, but separate language-specific from language-agnostic components so as to maximize adaptability to other contexts. To focus on problems and solutions specific to statistical software. 2.2 Related projects and initiatives The following internal and external projects related projects have (non-exclusive) bearing on our work: rOpenSci is simultaneously working on improving the automation and documentation of infrastructure to support software peer reviews. This will support many of the processes described herein, initially those in metrics and diagnostic reports (below), as well as in managing the review process. Secondly, under a separate project funded by the Moore Foundation, rOpenSci is building a system to automate the retrieval of information related to use of software in published literature and automate reporting of software impact as part of metadata in software repositories. This builds on Depsy and CiteAs projects and may be leveraged for our work on metrics (below). Third, an initiative organized under the R Consortium, the R Validation Hub, seeks to provide guidance primarily to R users in the pharmaceutical industry on validating R packages in regulated contexts. We are working closely with that group to share products and avoid duplicated efforts. 2.3 Outline of this document We now briefly summarise the four sections identified at the outset of this chapter, and which comprise Chapters 4–5 and 7–8 of this book: Scope; the Peer Review Process; Standards; and Software Assessment. 2.3.1 Scope of Statistical Software Review The scope of statistical software review considers the core task of defining the kinds of software that will be covered by our review process and standards, for which key questions are: What categories of statistical software might be considered in scope? What categories of statistical software might be considered out of scope? How would these vary between a useful definition and typology for general use, and the specific case of our R-focused peer review system? A key consideration in scope is identifying categories of software that (a) will benefit from our peer review process, and (b) the review process will be equipped to handle. That is, can standards and procedures be defined that are applicable, and will authors and reviewers be able to apply them? In considering these issues of definition, it will be important to consider whether it may be advantageous or necessary to develop different procedures for different sub-categories, whether in terms of categories of external form or categories of statistical software. It will likely be particularly important for the present project to develop a categorization scheme, particularly because the set of standards envisioned by this project will be variably applicable to different categories, and understanding which standards may or may not apply to a particular piece of software will provide important information for review purposes. Individual pieces of software will often fit more than one of these categories, and we envision relating some kind of categorical checklist directly to a corresponding checklist of relevant or applicable standards. Potential schemes for defining the scope of statistical software acceptable within our review process are considered in Chapter 4 below, with extensive consideration of potential categories in the second section of that chapter. 2.3.2 Standards for Statistical Software Chapter 5 considers Standards for Statistical Software intended to serve both as expectations against which to compare software, and as guides which reviewers may use to assess software. Important general questions regarding standards include the following: What kind of standards might apply to software in general? What kind of standards might specifically apply to statistical software? How might such standards differ between different languages? To what extent should we aim for “verification” or “validation” of software, and how might be signify such? 2.3.3 Software Assessment The Software Assessment section in Chapter 6 presents a general (yet non-exhaustive) overview of aspects of software which may be usefully considered for standards-based assessment, both for retrospective purposes of peer review, and for prospective use in developing software both in general, and in preparation for peer-review. 2.3.4 Statistical Software Peer Review Process Our point of departure for our process is the rOpenSci software peer review process, which has operated for five years, and has reviewed over &gt;200 packages, primarily in areas of data life cycle management. However, we aim to reassess this process in light of other models and needs specific to statistical software. Chapter 7 considers a few analogous processes of peer review, gleaning aspects which may be useful to adopt and adapt for our system. Chapter 8 then describes the peer review process in terms of the series of steps we currently envision comprising our system. 2.4 Community A core goal of the project is the building and maintenance of a community of practice that will facilitate dissemination, adoption, and improvement of standards and peer review. In striving for this goal, the following questions are important: What outreach should we conduct to maximize diversity and inclusion in the process? How should this process involve other relevant communities in fields including software development, statistics, applied statistics (in various subfields) What fora should we manage for developers, users, reviewers and editors to communicate? To what extent should we reuse existing fora from rOpenSci or other organizations? We now briefly consider the three aspects of community relevant to this project: communities of users, of developers, and of reviewers. Note that several of the kinds of “metrics” alluded to in the following lines are given explicit consideration at the end of this document. Software use and surrounding community: What sort of metrics might provide insight into community use of software? How might such community engagement be enhanced to improve such metrics? Software development and surrounding community: What sort of metrics might provide insight into community development of software? How might such community engagement be enhanced to improve such metrics? Reviewer pool and qualifications: What is the extent and type of effort expected of reviewers? To what extent might searches for suitable reviewers be automated? What sort of metrics might be useful in such searches? In each case the project will strive to cultivate diverse, inclusive, and geographically expansive communities, and metrics to describe such aspects may also be important, as may automated tools to monitor community engagement and development. Note that these aspects of community are not explicitly addressed throughout any of the remainder of this document. It is important the future revisions return to this point, and ensure that each of the following sections are appropriately modified to ensure effective consideration and incorporation of the concerns listed immediately above. "],
["reading.html", "Chapter 3 Some Light Reading: An Annotated Bibliography 3.1 Books 3.2 Journal Articles 3.3 Technical Reports 3.4 Computer Programs 3.5 Web Pages 3.6 Contributing to the bibliography", " Chapter 3 Some Light Reading: An Annotated Bibliography Below is an annotated bibliography of sources that have been useful in developing this document and the statistical software peer-review standards. Note this page is auto-generated from a Zotero collection. See Contributing to the Bibliography below to add more sources. 3.1 Books There is an enormous wealth of books published on software review, software assessment (often referred to via concepts of “validation” and/or “verification”), and related concepts. Most importantly in the present context, almost all such books address closed-source, proprietary software, with no published books specifically dedicated to review or assessment of open-source software. Software testing automation tips: 50 things automation engineers should know. Alpaev, Gennadiy ( 2017 ). A neat list of just what the title suggests, grouped into general topics of Scripting, Testing, Environment, Running Logging Verifying, and Reviewing. Introduction to software testing. Ammann, Paul; and Offutt, Jeff ( 2017 ). Has useful definitions of Verification as “The process of determining whether the products of a phase of the software development process fulfill the requirements established during the previous phase;” and  Validation as “The process of evaluating software at theend of software development to ensure compliance with intended usage.”   Also includes extensive consideration of testing, in particular chapters on “Model-driven test design” (2), and “Writing Test Plans” (11).   Software Verification and Validation: An Engineering and Scientific Approach. Fisher, Marcus S. ( 2007 ). Really useful considerations of risk as likelihood times consequence, with extensive worked examples of how these factors might be scaled (with most examples on simple four-point scales which prove to be sufficient to yield fruitful insight).   Transparent Statistics Guidelines. Group (http://transparentstatistics.org/), Transparent Statistics in HCI Working ( ). Currently only just begun, but aims to work towards what might be a very useful guide on how statistics might best be reported. Software Quality Approaches: Testing, Verification, and Validation: Software Best Practice 1. Haug, Michael; Olsen, Eric W; and Consolini, Luisa ( 2001 ). A huge EU project, drawn from results of a host of “Process Improvement Experiments” (PIEs), all of which were controlled experiments built on a generic model, and involved manipulating some standard baseline practice and analysing effects.   Software testing: concepts and operations. Mili, Ali ( 2015 ). Describes some problems facing modern software development (p9), including “Absence of Reuse Practice”, and lack of standard software architecture. Neither of these are applicable to R packages. Then goes on to describe (p11) “The Absence of Automation”, and “Limited Quality Control”, both of which are definitely still applicable. The art of software testing. Myers, Glenford J; Badgett, Tom; and Sandler, Corey ( 2012 ). Glenford Myers originally published “The Art of Software Testing” in 1979. Most of the book is either pretty straightforward, or overly specific to be able to be directly transferred or translated to open source software.   Conventions for R Modeling Packages. tidyverse ( ). “The goal of this document is to define a specification for creating functions and packages for new modeling packages . These are opinionated specifications but are meant to reflect reasonable positions for standards based on prior experience. A number of these guidelines are specific to the tidyverse (e.g. “Function names should use snake_case instead of camelCase.”). However, the majority are driven by common sense and good design principles (e.g. “All functions must be reproducible from run-to-run.”).\" Medical device software verification, validation and compliance. Vogel, David A ( 2011 ). In spite of the applied focus, a highly relevant and insightful book. Of particular relevance are the extensive considerations of software lifecycles, verification and validation and associated metrics, and considerations of software testing in relation to the US FDA’s General Principles of Software Validation.0 The tidyverse style guide. Wickham, Hadley ( ). “All style guides are fundamentally opinionated. Some decisions genuinely do make code easier to use (especially matching indenting to programming structure), but many decisions are arbitrary. The most important thing about a style guide is that it provides consistency, making code easier to write because you need to make fewer decisions.” 3.2 Journal Articles Reconciling modern machine learning practice and the bias-variance trade-off. Belkin, Mikhail; Hsu, Daniel; Ma, Siyuan; and Mandal, Soumik ( 2019-09-10 ). arXiv:1812.11118 [cs, stat] A very good reference for the ubiquity of over-fitting in machine learning algorithms, which is nevertheless mostly dedicated to demonstrating a very practical approach to overcoming over-fitting. Datasheets for Datasets. Gebru, Timnit; Morgenstern, Jamie; Vecchione, Briana; Vaughan, Jennifer Wortman; Wallach, Hanna; Daumeé III, Hal; and Crawford, Kate ( 2019-04-14 ). arXiv:1803.09010 [cs] Comment: Working Paper, comments are encouraged Four simple recommendations to encourage best practices in research software. Jiménez, Rafael C.; Kuzak, Mateusz; Alhamdoosh, Monther; Barker, Michelle; Batut, Bérénice; Borg, Mikael; Capella-Gutierrez, Salvador; Chue Hong, Neil; Cook, Martin; Corpas, Manuel; Flannery, Madison; Garcia, Leyla; Gelpí, Josep Ll.; Gladman, Simon; Goble, Carole; González Ferreiro, Montserrat; Gonzalez-Beltran, Alejandra; Griffin, Philippa C.; Grüning, Björn; Hagberg, Jonas; Holub, Petr; Hooft, Rob; Ison, Jon; Katz, Daniel S.; Leskošek, Brane; López Gómez, Federico; Oliveira, Luis J.; Mellor, David; Mosbergen, Rowland; Mulder, Nicola; Perez-Riverol, Yasset; Pergl, Robert; Pichler, Horst; Pope, Bernard; Sanz, Ferran; Schneider, Maria V.; Stodden, Victoria; Suchecki, Radosław; Svobodová Vařeková, Radka; Talvik, Harry-Anton; Todorov, Ilian; Treloar, Andrew; Tyagi, Sonika; van Gompel, Maarten; Vaughan, Daniel; Via, Allegra; Wang, Xiaochuan; Watson-Haigh, Nathan S.; and Crouch, Steve ( 2017-6-13 ). F1000Research: 876 The four recommendations are: Make source code publicly accessible from day one Make software easy to discover by providing software metadata via a popular community registry Adopt a licence and comply with the licence of third-party dependencies Define clear and transparent contribution, governance, and communication processes Towards FAIR principles for research software. Lamprecht, Anna-Lena; Garcia, Leyla; Kuzak, Mateusz; Martinez, Carlos; Arcila, Ricardo; Martin Del Pico, Eva; Dominguez Del Angel, Victoria; van de Sandt, Stephanie; Ison, Jon; Martinez, Paula Andrea; McQuilton, Peter; Valencia, Alfonso; Harrow, Jennifer; Psomopoulos, Fotis; Gelpi, Josep Ll; Chue Hong, Neil; Goble, Carole; and Capella-Gutierrez, Salvador ( 2019/01/01 ). Data Science (Preprint): 1-23 An important reference for adapting FAIR (Findability, Accessibility, Interoperability, and Reusability) principles to software. All of these principles are effectively already met by rOpenSci’s current review system, noting in particular that the majority of them pertain to the creation, existence, and curation of metadata. Data Management Lifecycle and Software Lifecycle Management in the Context of Conducting Science. Lenhardt, W.; Ahalt, Stanley; Blanton, Brian; Christopherson, Laura; and Idaszak, Ray ( 2014-07-09 ). Journal of Open Research Software (1): e15 A relatively brief exploration of parallels between relatively well-established notions of lifecycle in regard to data management and curation, and less-established considerations of lifecycle in regard to software. Primarily an argument curation of appropriate metadata, it makes frequent reference to a variety of ISO standards. 3.3 Technical Reports Software Evaluation Guide | Software Sustainability Institute. Jackson, Mike; Crouch, Steve; and Baxter, Rob ( 2011 ). Divides considerations into the following categories and sub-categories:   Usability     - 1.1 Understandability     - 1.2 Documentation     - 1.3 Learnability     - 1.4 Buildability     - 1.5 Installability     - 1.6 Performance Sustainability &amp; Maintainability     - 2.1 Identity     - 2.2 Copyright &amp; Licensing     - 2.3 Accessibility     - 2.4 Community     - 2.5 Testability     - 2.6 Portability     - 2.7 Supportability     - 2.8 Analysability     - 2.9 Changeability     - 2.10 Reusability     - 2.11 Security &amp; Privacy     - 2.12 Interoperability     - 2.13 Governance   The main document divides each of these into numerous explicit categories, with the resultant document serving as likely a useful comparison or contrast to the Core Infrastructure Best Practices checklist. Automated Source Code CISQ Maintainability Measure Specification Version 1.0. Objcect Management Group ( 2016 ). Considers a number of very specific metrics, including the following: Control Flow Transfer Control Element outside Switch Block Class Element ExcessiveInheritance of Class Elements with Concrete Implementation Storable and Member DataElement Initialization with Hard-Coded Literals Callable and Method ControlElement Number of Outward Calls Loop Value Update within the Loop Commented-out Code Element Excessive Volume (with default threshold of 2%) Inter-Module Dependency Cycles Source Element Excessive Size (with default of 1,000 lines per file) Horizontal Layer Excessive Number (with default of 8) Named Callable and Method Control Element Multi-Layer Span (“Avoid unclear allocation of software elements to a single architectural layer”) Callable and Method Control Element Excessive Cyclomatic Complexity Value (with default threshold of 20). Named Callable and Method Control Element with Layer-Skipping Call (Aim: Avoid calls from an upper layer to lower layers that are not adjacent). Callable and Method Control Element Excessive Number of Parameters (with default threshold of 7) Callable and Method Control Element Excessive Number of Control Elements Involving Data Element from Data Manager or File Resource (Aim: Reduce numbers of external data dependencies, with default threshold of 7). Public Member Element Method Control Element Usage of Member Element from other Class Element Class Element Excessive Inheritance Level (with default threshold of 7) Class Element Excessive Number of Children (with default threshold of 10) Named Callable and Method Control Element Excessive Similarity (based on assessment of numbers of identical compiled tokens within a unit) Unreachable Named Callable or Method Control Element (Aim: Avoid inactive code blocks) ESIP Software Guidelines Draft. Scott, Soren ( 2016 ). Earth Science Information Partners (ESIP) guidelines, divided among the primary categories of Sustainable Code     1.1 Clean, standardized code     1.2 Versioned code     1.3 Redistributable     1.4 Tested Interoperable Usable     3.1 Clear, understandable interface     3.2 Performant and stable Documented     4.1 Source code is documented     4.2 Codebase includes documentation to support adaptation and reuse     4.3 Software is documented     4.4 Code or software requirements are documented     4.5 Project is documented Secure Shareable Governed     7.1 Contribution policies are provided     7.2 Development activities are transparent Progression, sustainability, and reusability/adoption CLARIAH/software-quality-guidelines. van Gompel, Maarten; Noordzij, Jauco; de Valk, Reinier; and Scharnhorst, Andrea ( 2016 ). Divides considerations into the categories and sub-categories derived by the Software Sustainability Institute in 2011. Each of these has several explicit sub-components, with each of them scored on a 5-point scale. The entire document provides an outstandingly strong and usable reference or benchmark from which a set of standards for software quality might be adapted. 3.4 Computer Programs lifecycle: Manage the Life Cycle of your Package Functions. Henry, Lionel; and RStudio, ( 2020-03-06 ). A good potential candidate for recommended ways to manage software lifecycles RWsearch: Lazy Search in R Packages, Task Views, CRAN, the Web. All-in-One Download. Kiener, Patrice ( 2020-02-15 ). May be useful for it’s ability to download and search package documentation. microsoft/nni. Microsoft ( 2020-03-16T12:25:52Z ). Potentially useful as a benchmarking tool for ML implementations based on Neural Networks (see also uber/ludwig) The Turing Way: A Handbook for Reproducible Data Science. The Turing Way Community, ; Becky Arnold, ; Louise Bowler, ; Sarah Gibson, ; Patricia Herterich, ; Rosie Higman, ; Anna Krystalli, ; Alexander Morley, ; Martin O’Reilly, ; and Kirstie Whitaker, ( 2019-03-25 ). A useful reference for various aspects of reproducible science uber/ludwig. uber ( 2020-03-16T10:25:42Z ). Potentially useful as a benchmarking tool for ML implementations (see also microsoft/nni) 3.4.1 Computer Programs (Testing) Software specifically designed for testing. mikldk/roxytest. Andersen, Mikkel Meyer ( 2020-03-04T21:43:17Z ). Very useful extension of R testing through enabling tests to be specified in-line via roxygen entries, either through explicit specification (via @test hooks), or through converting examples to tests (via @testexamples hooks). markvanderloo/tinytest. Loo, Mark van der ( 2020-03-04T15:20:41Z ). Implements the unique (in R) and useful approach of treating test results as data amenable to analysis, rather than just throwing interrupt signals. Most testing is ineffective - Hypothesis. MacIver, David ( ). “Hypothesis is a family of testing libraries which let you write tests parametrized by a source of examples. A Hypothesis implementation then generates simple and comprehensible examples that make your tests fail. This simplifies writing your tests and makes them more powerful at the same time, by letting software automate the boring bits and do them to a higher standard than a human would, freeing you to focus on the higher level test logic.” LudvigOlsen/xpectr. Olsen, Ludvig Renbo ( 2020-03-02T09:31:14Z ). R package for generating expectations for testthat unit testing. The main utility is via functions which automatically generate tests appropriate to the input structure of functions. r-lib/waldo. Wickham, Hadley ( 2020-03-30T13:36:53Z ). Uses diffobj to create and return inline diffs between objects. Likely to be particularly useful in tests. 3.5 Web Pages Jakob Bossek - PhD candidate. Bossek, Jakob ( ). List of software packages developed by Jakob Bossek, most of which are devoted to providing benchmarking abilities for comparison of graphs and graph algorithms. coreinfrastructure/best-practices-badge. Core Infrastructure ( ). A definitive reference for general standards in open-source software, as detailed in the doc/criteria.md document. That said, it is a github-based document, and many of the standards directly describe github-based attributes and workflows.   Sections considered are: Basic Website License Documentation Other Must use https Must provide for discussion Must be in English Change Control Public version control repo Unique version numbering Semantic versioning Release notes Reporting Bug reporting Vulnerability reporting Quality Working build system Automated test suite (cover most branches, input fields, and functionality) Continuous integration New functionality testing Warning flags / linter Security Analysis Static code analysis (with links to lots of language-specific tools) Dynamic code analysis (with links to lots of language-specific tools) Use of memory check tools, or memory-safe languages testing statistical software. Hayes, Alex ( ). Discusses four types of tests for statistical software: Correctness Tests Parameter Recovery Tests Convergence Tests Identification Tests (uniqueness/stability of solution) CODECHECK process. Nüst, Stephen Eglen &amp; Daniel ( ). A badging service indicating whether or not code used to support published scientific manuscripts is reproducible or not. A Risk-based Approach for Assessing R package Accuracy within a Validated Infrastructure. R Validation HUb ( ). The White Paper from the R Validation Hub project for validation pharmaceutical software. “R Validation Hub is a cross-industry initiative whose mission is to enable the use of R by the Bio-Pharmaceutical Industry in a regulatory setting, where the output may be used in submissions to regulatory agencies.” It describes the motivations and principles of their risk-based assessment of statistical software, much of which is directly implemented in the riskmetric package. Pull Request review process - Reside-IC. Reside-IC ( ). A useful description of the pull request workflow developed by the Research Software for Infection Disease Epidemiology group at Imperial College, London. Shields.io: Quality metadata badges for open source projects. sheilds.io ( ). shields.io provides a badging service for many aspects of code, divided into the following general categories: Build Code coverage Analysis Chat Dependencies Size Downloads Funding Issue Tracking License Rating Social Version Platform &amp; Version Support Monitoring Activity PEP 8 – Style Guide for Python Code. van Rossum, Guido; Warsaw, Barry; and Coghlan, Nick ( ). The most widely used style guide for python code 3.6 Contributing to the bibliography This annotated bibliography is automatically generated from entries containing “Note” fields in the zotero library accompanying this project. Please feel free to add any additional entries you may see fit to this open library according to the following steps: Open your local zotero client and get a local copy of the project library; Enable zotero in your web browser so that you can click on any webpage to add an entry to the shared library; Manually add a “Note” to any new entries and they’ll automatically appear here the next time this page is updated. This annotated bibliography extracts all text in all Note fields up to the first markdown section break, itself defined by a single line with a least three consecutive dashes as in the following example: Add some note text to appear in this annotated bibliography --- This text below the three dashes and any subsequent lines will not appear here. Notes which do not contain a section break will appear here in their entirety. "],
["scope.html", "Chapter 4 Scope [SEEKING FEEDBACK] 4.1 Software types 4.2 Statistical Categories 4.3 Proposals", " Chapter 4 Scope [SEEKING FEEDBACK] One task in extending the rOpenSci peer review system to statistical software is defining scope - what software is included or excluded. Defining scope requires some grouping of packages into categories. These categories play key roles in the peer review process and standards-setting. Categorical definitions can determine which kinds of software will be admitted; Different categories of software will be subject to different standards, so categories are key to developing standards, review guidance, and automated testing. Creating a categorization or ontology of statistical software can easily become an overwhelming project in itself. Here we attempt to derive categories or descriptors which are practically useful in the standards and review process, rather than a formally coherent system. We use a mix of empirical research on common groupings of software and subjective judgement as to their use in the review process. We consider two main types of categories: Categories of software structure, referred to as “software types”, determined by computer languages and package formats in those languages; and Categories defining different types of statistical software, referred to as “statistical categories”. 4.1 Software types 4.1.1 Languages This project extends existing an software peer-review process run by rOpenSci, and is primarily intended to target the R language. Nonetheless, given the popularity of Python in the field (see relevant analyses and notes in Appendix A), the impact of developing standards applicable to Python packages must be considered. rOpenSci also has a close collaboration with its sister organization, pyOpenSci. In addition it is particularly important to note that many R packages include code from a variety of other languages. The following table summarises statistics for the top ten languages from all 15,576 CRAN packages as of Tue May 05 2020 (including only code from the /R, /src, and /inst directories of each package). Table 4.1: Proportion of code lines in different languages in all CRAN packages. language lines proportion R 21,687,052 0.451 C/C++ Header 6,438,969 0.134 HTML 4,774,583 0.099 C 4,723,903 0.098 C++ 4,391,283 0.091 JavaScript 1,270,762 0.026 Fortran 77 798,821 0.017 JSON 685,641 0.014 CSS 570,436 0.012 Rmd 489,634 0.010 Close to one half of all code in all R packages to date has been written in the R language, clearly justifying a primary focus upon that language. Collating all possible ways of packaging and combining C and C++ code yields 15,554,155 lines or code or 32% of all code, indicating that 77% of all code has been written in either R or C/C++. Three of these top ten languages are likely related to web-based output (HTML, JavaScript, and CSS), representing a total of 14% of all code. While this is clearly a significant proportion, and while this may reflect an equivalent high frequency of code devoted to some form of web-based visualisation, these statistics represent all R packages. In many cases this represents extensive headers in supplementary documentation. There is no simple way to identify which of these might be considered statistical code in web-based languages, but knowing that there are packages exclusively constructed to generate web-based visualisations and documentation in a generic sense suggests that this value may be taken as an upper limit on the likely frequency of these types of visualisation packages (or parts thereof) in the context of statistical software. Key considerations: Expansion into the Python ecosystem has great potential for impact, but goes beyond the general areas of expertise in the core ecosystem. (And Python code represents just 162,339 lines of code, or 0.3% of all code within all R packages.) Compiled languages within R packages are core to many statistical applications; excluding them would exclude core functionality the project aims to addressed. The majority of compiled code is nevertheless C and/or C++, with Fortran representing under 2% of all code. Languages used for web-based visualisations comprise a significant proportion (14%) of all code. While this potentially indicates a likely importance of visualisation routines, this figure reflects general code in all R packages, and the corresponding proportion within the specific context of statistical software may be considerably lower. Any decision to include visualisation software and routines within our scope will likely entail an extension of linguistic scope to associated languages (HTML, JavaScript, and maybe CSS). 4.1.2 Structure R has a well-defined system for structuring software packages\" Other forms of packaging R software may nevertheless be considered within scope. These may include Python-like systems of modules for R; Packaging appropriate for other languages (such as Python) yet with some interface with the R language; R interfaces (“wrappers”) to algorithms or software developed independently in different languages, and which may or may not be bundled as a standard R package; and Web applications such as Shiny packages. Key considerations: Allowing non-package forms of code into the peer review system could potentially bring in a large pool of code typically published alongside scientific manuscripts, and web applications are a growing, new area of practice. However, there is far less standardization of code structure to allow for style guidelines and automated testing in these cases. 4.2 Statistical Categories As alluded to at the outset of this chapter, a primary task of this project will be to categorise statistical software in order to: Determine the extent to which software fits within scope Enable fields of application of software to be readily identified Enable determination of applicable standards Enable discernment of appropriate reviewers Different categories of statistical software will likely have different standards, yet there will nevertheless be general standards applicable regardless of categories. 4.2.1 Examples of Statistical Software We now consider a few brief categorical examples, to illustrate the kinds of decisions such a process of categorisation will likely face. gtsummary, submitted to rOpenSci and reject as out-of-scope. Creates presentation-ready tables summarizing data sets, regression models, and more. The code to create the tables is concise and highly customizable. Data frames can be summarized with any function, e.g. mean(), median(), even user-written functions. Regression models are summarized and include the reference rows for categorical variables. Common regression models, such as logistic regression and Cox proportional hazards regression, are automatically identified and the tables are pre-filled with appropriate column headers. This package appears not to contain any algorithmic implementations, yet is clearly aimed at enhancing a purely statistical workflow. Such a submission requires answering the question of whether software categorized as “workflow” only and which does not correspond to any other of the above categories, may be deemed in scope? greta: simple and scalable statistical modelling in R, published in JOSS. greta is an package for statistical modelling in R (R Core Team, 2019) that has three core differences to commonly used statistical modelling software packages: greta models are written interactively in R code rather than in a &gt; compiled domain specific language. greta can be extended by other R packages; providing a fully-featured &gt; package management system for extensions. greta performs statistical inference using TensorFlow (Abadi et al., &gt; 2015), enabling it to scale across modern high-performance computing &gt; systems. The greta package might be considered predominantly an interface to TensorFlow, yet it provides a new way to specify and work with purely statistical models. This might be considered under both workflow and wrapper categories, and serves here to illustrate the question of whether wrappers around, in this case, externally-installed software might be considered in scope? And if so, to what extent ought aspects of such externally-installed software also be directly addressed within a review process? modelStudio, published in JOSS. The modelStudioR package automates the process of model exploration. It generates advanced interactive and animated model explanations in the form of a serverless HTML site. It combines R(R Core Team, 2019) with D3.js (Bostock, 2016) to produce plots and descriptions for various local and global explanations. Tools for model exploration unite with tools for EDA to give a broad overview of the model behaviour. As with gtsummary above, this is clearly a package intended to enhance a workflow, and furthermore one which primarily serves to generate summary output as a ht``ml document, yet the models it considers, and all aspects of output produced, are purely statistical. This package could meet both workflow and visualization categories, and serves here to illustrate difficulties in considering the latter of these. The D3.``js library contains numerous indubitably statistical routines, and so this package might be argued to be a wrapper in the same category as greta is a wrapper around TensorFlow. An important question likely to arise in considering both of these is the extent to which the library being wrapped should also be predominantly statistical for a package to be in scope? (A requirement which greta would more easily fulfil than gtsummary.) We now consider potential categories within the general domain of statistical software. In order to derive a realistic categorisation, we used empirical data from several sources of potential software submissions, including all apparently “statistical” R packages published in the Journal of Open Source Software (JOSS), packages published in the Journal of Statistical Software, software presented at the 2018 and 2019 Joint Statistical Meetings (JSM), and Symposia on Data Science and Statistics (SDSS), well as CRAN task views. We have also compiled a list of the descriptions of all packages rejected by rOpenSci as being out of current scope because of current inability to consider statistical packages, along with a selection of recent statistical R packages accepted by JOSS. (The full list of all R package published by JOSS can be viewed at https://joss.theoj.org/papers//in/R). We allocated one or more key words (or phrases) to each abstract, and use the frequencies and inter-connections between these to inform the following categorisation are represented in the interactive graphic (also included in the Appendix), itself derived from analyses of abstracts from all statistical software submitted to both rOpenSci and JOSS. (Several additional analyses and graphical representations of these raw data are included an auxiliary github repository.) The primary nodes that emerge from these empirical analyses (with associated relative sizes in parentheses) are shown in the following table. Table 4.2: Most frequent key words from all JOSS abstracts (N = 92) for statistical software. Proportions are scaled per abstract, with each abstract generally having multiple key words, and so sum of proportions exceeds one. n term proportion 1 ML 0.133 2 statistical indices and scores 0.111 3 visualization 0.111 4 dimensionality reduction 0.100 5 probability distributions 0.100 6 regression 0.100 7 wrapper 0.100 8 estimates 0.089 9 Monte Carlo 0.089 10 Bayesian 0.078 11 categorical variables 0.078 12 EDA 0.078 13 networks 0.078 14 summary statistics 0.067 15 survival 0.067 16 workflow 0.067 The top key words and their inter-relationships within the main network diagram were used to distinguish the following primary categories representing all terms which appear in over 5% of all abstracts, along with the two additional categories of “spatial” and “education”. We have excluded the key word “Estimates” as being too generic to usefully inform standards, and have also collected a few strongly-connected terms into single categories. Table 4.3: Proposed categorisation of statistical software, with corresponding proportions of all JOSS software matching each category n term proprtion comment 1 Bayesian &amp; Monte Carlo 0.167 2 dimensionality reduction &amp; feature selection 0.144 Commonly as a result of ML algorithms 3 ML 0.133 4 regression/splines/interpolation 0.133 Including function data analysis 5 statistical indices and scores 0.111 Software generally intended to produce specific indices or scores as statistical output 6 visualization 0.111 7 probability distributions 0.100 Including kernel densities, likelihood estimates and estimators, and sampling routines 8 wrapper 0.100 9 categorical variables 0.078 Including latent variables, and those output from ML algorithms. Note also that method for dimensionality reduction (such as clustering) often transform data to categorical forms. 10 Exploratory Data Analysis (EDA) 0.078 Including information statistics such as Akaike’s criterion, and techniques such as random forests. Often related to workflow software. 11 networks 0.078 12 summary statistics 0.067 Primarily related in the empirical data to regression and survival analyses, yet clearly a distinct category of its own. 13 survival 0.067 strongly related to EDA, yet differing in being strictly descriptive of software outputs whereas EDA may include routines to explore data inputs and other pre-output stages of analysis. 14 workflow 0.067 Often related to EDA, and very commonly also to ML. 15 spatial 0.033 Also an important intermediate node connecting several other nodes, yet defining its own distinct cluster reflecting a distinct area of expertise. 16 education 0.044 The full network diagram can then be reduced down to these categories only, with interconnections weighted by all first- and second-order interconnections between intermediate categories, to give the following, simplified diagram (in which “scores” denotes “statistical indices and scores”; with the diagram best inspected by dragging individual nodes to see their connections to others). We intend, at least initially, to use these categories to define and guide the assessment of statistical software. Standards considered under any of the ensuing categories must be developed with reference to inter-relationships between categories, and in particular to potential ambiguity within and between any categorisation. An example of such ambiguity, and of potential difficulties associated with categorisation, is the category of “network” software which appropriate describes the grapherator package (with accompanying JOSS paper) which is effectively a distribution generator for data represented in a particular format that happens to represent a graph; and three JSM presentations, one on network-based clustering of high-dimensional data, one on community structure in dynamic networks and one on Gaussian graphical models. Standards derived for network software must accommodate such diversity of applications, and must accommodate software for which the “network” category may pertain only to some relatively minor aspect, while the primary algorithms or routines may not be related to network software in any direct way. 4.2.2 Bayesian and Monte Carlo Routines Packages implementing or otherwise relying on Bayesian or Monte Carlo routines represent form the central “hub” of all categories in the above diagram, indicating that even though this category is roughly equally common to other categories, software in this category is more likely to share more other categories. In other words, this is the leading “hybrid” category within which standards for all other categories must also be kept in mind. Some examples of software in this category include: The bayestestR package “provides tools to describe … posterior distributions” The ArviZ package is a python package for exploratory analyses of Bayesian models, particularly posterior distributions. The GammaGompertzCR package features explicit diagnostics of MCMC convergence statistics. The BayesianNetwork package is in many ways a wrapper package primarily serving a shiny app, but also accordingly a package in both education and EDA categories. The fmcmc package is a “classic” MCMC package which directly provides its own implementation, and generates its own convergence statistics. The rsimsum package is a package to “summarise results from Monte Carlo simulation studies”. Many of the statistics generated by this package may prove useful in assessing and comparing Bayesian and Monte Carlo software in general. (See also the MCMCvis package, with more of a focus on visualisation.) The walkr package for “MCMC Sampling from Non-Negative Convex Polytopes” is indicative of the difficulties of deriving generally applicable assessments of software in this category, because MCMC sampling relies on fundamentally different inputs and outputs than many other MCMC routines. Key Considerations The extent to which the output of Bayesian routines with uninformative prior inputs can or do reflect equivalent frequentist analyses. Ways to standardise and compare diagnostic statistics for convergence of MCMC routines. Forms and structures of data using in these routines are very variable, likely making comparison among algorithms difficult. 4.2.3 Dimensionality Reduction and Feature Selection Many packages either implement or rely upon techniques for dimensionality reduction or feature selection. One of the primary problems presented by such techniques is that they are constrained to yield a result independent on any measure of correctness of accuracy (Estivill-Castro 2002). This can make assessment of the accuracy or reliability of such routines difficult. Moreover, dimensionality reduction techniques are often developed for particular kinds of input data, reducing abilities to compare and contrast different implementations, as well as to compare them with any notional reference implementations. ivis implements a dimensionality reduction technique using a \"Siamese Neural Network architecture. tsfeaturex is a package to automate “time series feature extraction,” which also provides an example of a package for which both input and output data are generally incomparable with most other packages in this category. iRF is another example of a generally incomparable package within this category, here one for which the features extracted are the most distinct predictive features extracted from repeated iterations of random forest algorithms. compboost is a package for component-wise gradient boosting which may be sufficient general to potentially allow general application to problems addressed by several packages in this category. The iml package may offer usable functionality for devising general assessments of software within this category, through offering a “toolbox for making machine learning models interpretable” in a “model agnostic” way. Key Considerations It is often difficult to discern the accuracy of reliability of dimensionality reduction techniques. It is difficult to devise general routines to compare and assess different routines in this category, although possible starting points for the development of such may be offered by the compboost and iml packages. 4.2.4 Machine Learning Machine Learning (ML) routines play a central role in modern statistical analyses, and the ML node in the above diagram is roughly equally central, and equally connected, to the Bayesian and Monte Carlo node. Machine Learning algorithms represent perhaps some of the most difficult algorithms for which to develop standards and methods of comparison. Both input and output data can be categorically different or even incomparable, while even where these may be comparable, the abiding aims of different ML algorithms can differ sufficiently to make comparison of outputs to otherwise equivalent inputs largely meaningless. A few potentially fruitful routes towards productive comparison may nevertheless be discerned, here according to the sub-domains of input data, output data, and algorithms. Input Data One promising R package which may prove very useful for standardising and comparing data used as input to ML algorithms is the vtreat package that “prepares messy real world data for predictive modeling in a reproducible and statistically sound manner.” The routines in this package perform a series of tests for general sanity of input data, and may prove generally useful as part of a recommended ML workflow. Algorithms A number of packages attempt to offer unified interfaces to a variety of ML algorithms, and so may be used within the context of the present project either as potential recommended standards, or as ways by which different algorithms may be compared within a standard workflow. Foremost among such packages are mlr3, which represents one of the core R packages for ML, developed by the key developers of previous generations of ML software in R. It offers a modular and extensible interface for a range of ML routines, and may prove very useful in comparing different ML routines and implementations. Output Data There are several extant packages for (post-)processing data output from ML algorithms. Many, perhaps even most, of these primarily aim to derive insightful visualisations of output, whether in interactive (JavaScript-based) form, as with the modelStudio or modelDown packages, or more static plots using internal graphical routines from R, as in the iml (Interpretable Machine Learning) package. The latter package offers a host of additional functionality useful in interpreting the output of ML algorithms, and which may prove useful in general standards-based contexts. Potential “edge cases” which may be difficult to reconcile with the general aspects described above include the following: ReinforcementLearning is a simulation package employing ML routines to enable agents to learn through trial and error. It is an example of a package with inputs and outputs which may be difficult to compare with other ML software, and difficult to assess via general standards. BoltzMM is an implementation of a particular class of ML algorithms (“Boltmann Machines”), and so provides an obverse example to the above, for which in this case inputs and outputs may be compared in standard ways, yet the core algorithm may be difficult to compare. dml is a collection of different ML algorithms which perform the same task (“distance metric learning”). While comparing algorithms within the package is obviously straightforward, comparison in terms of external standards may not be. 4.2.5 Regression and Interpolation This category represents the most important intermediate node in the above network graphic between ML and Bayesian/Monte Carlo algorithms, as well as being strongly connected to several other nodes. While many regression or interpolation algorithms are developed as part of general frameworks within these contexts, there are nevertheless sufficiently many examples of regression and interpolation algorithms unrelated to these contexts to warrant the existence of this distinct category. That said, algorithms within this category share very little in common, and each implementation is generally devised for some explicit applied purpose which may be difficult to relate to any other implementations in this category. Perhaps one feature which almost of the following examples share in common is input and output data in (potentially multi-dimensional) vector format, very generally (but not exclusively) in numeric form. This may be one category in which the development of a system for property-based testing, like the hypothesis framework for python may be particularly useful. Such a system would facilitate tests in response to a range of differently input structures, such as values manifesting different distributional properties. Property-based testing is likely to be a particularly powerful technique for uncovering faults in regression and interpolation algorithms. Examples of the diversity of software in this category include the following. xrnet to perform “hierarchical regularized regression to incorporate external data”, where “external data” in this case refers to structured meta-data as applied to genomic features. survPen is, “an R package for hazard and excess hazard modelling with multidimensional penalized splines” areal is, “an R package for areal weighted interpolation”. ChiRP is a package for “Chinese Restaurant Process mixtures for regression and clustering”, which implements a class of non-parametric Bayesian Monte Carlo models. klrfome is a package for, “kernel logistic regression on focal mean embeddings,” with a specific and exclusive application to the prediction of likely archaeological sites. gravity is a package for “estimation methods for gravity models in R,” where “gravity models” refers to models of spatial interactions between point locations based on the properties of those locations. compboost is an example of an R package for gradient boosting, which is inherently a regression-based technique, and so standards for regression software ought to consider such applications. ungroup is, “an R package for efficient estimation of smooth distributions from coarsely binned data.” As such, this package is an example of regression-based software for which the input data are (effectively) categorical. The package is primarily intended to implement a particular method for “unbinning” the data, and so represents a particular class of interpolation methods. registr is a package for “registration for exponential family functional data,” where registration in this context is effectively an interpolation method applied within a functional data analysis context. One package which may be potential general use is the ggeffects package for “tidy data frames of marginal effects from regression models.” This package aims to make statistics quantifying marginal effects readily understandable, and so implements a standard (tidyverse-based) methodology for representing and visualising statistics relating to marginal effects. 4.2.6 Statistical Indices and Scores Many packages are designed to provide one or more specific statistical indices, scores, or summary statistics from some assumed type of input data. Methodology used to derive indices or scores may draw on many of the methods or algorithms considered in the first category above or are often field-specific, arithmetic calculations. Such software may likely be considered within its own category through a singular aim to provide particular indices or scores, in contrast with more generic “Methods and Algorithms” software which offers more abstraction or modeled approach. Some examples include: The spatialwarnings package which provides “early-warning signal of ecosystem degradation,” where these signals and associated indices are highly domain-specific. The heatsaveR package which calculates and displays marine heatwaves using specific indices established in previously-published literature. The hhi package which calculates and visualizes “Herfindahl-Hirschman Index Scores,” which are measures of numeric concentration. The DscoreApp package which provides an index (the “D-Score”) to quantify the results of Implicit Association Tests. The thurstonianIRT package (with accompanying JOSS paper) for score forced-choice questionnaires using “Item Response Theory”. Key Considerations: Such packages can generally be reviewed for correctness (or accuracy/precision) in comparison to pseudocode, reference implementations, or reference data sets and in this way have can be straightforwardly evaluated. More complex indices and scores will require many of the considerations in the “methods and algorithms” category above. In many cases, the field-specific nature of indices and scores may tightly tie the algorithm implementation to certain data input formats or workflows common to practitioners. They may have considerable overlap with workflow packages (below). There is also the possibility that some indices could be considered “trivial” arithmetic calculations. We may wish to consider some qualitative standard for additional utility that such packages would provide. 4.2.7 Visualisation While many may consider software primarily aimed at visualisation to be out of scope, there are nevertheless cases which may indeed be within scope, notably including the ggfortify package which allows results of statistical tests to be “automatically” visualised using the ggplot2 package. The list of “fortified” functions on the packages webpage clearly indicates the very predominantly statistical scope of this software which is in effect a package for statistical reporting, yet in visual rather than tabular form. Other examples of visualisation software include: The modelStudio package (with accompanying JOSS paper), which is also very much a workflow package. The shinyEFA package (with accompanying JOSS paper) which provides a, “User-Friendly Shiny Application for Exploratory Factor Analysis.” The autoplotly package (with accompanying JOSS paper) which provides, “Automatic Generation of Interactive Visualisations for Statistical Results”, primarily by porting the output of the authors’ above-mentioned ggfortify package to plotly.js. Key considerations: The quality or utility visualization techniques can be strongly subjective, but also may be evaluated using standardized principles if the community can come to a consensus on those principles. Such considerations may be context-dependent - e.g., the requirements of a diagnostic plot designed to support model-checking are different from that designed to present raw data or model results to a new audience. This implies that the intended purpose of the visualization should be well-defined. Whether or not visualization is in-scope, many software packages with other primary purposes also include functions to visualise output. Visualization will thus never be strictly out of scope. However one option is not to include primarily visualization packages, or only statistical visualization packages in which visualization is closely tied to another category or purpose. Visualisation packages will include numerical or statistical routines for transforming data from raw form to graphics, which can be evaluated for correctness or accuracy. 4.2.8 Probability Distributions The category of probability distributions is an outlier in the preceding network diagram, connected only to ML and regression/interpolation algorithms. The latter category was identified as one in which property-based testing was likely to be useful, within similar suggestions applying to the present category, particularly through enabling routines to be tested for robustness against a variety of perturbations to assumed distributional forms. Packages which fall exclusively within this category and not within any of the other categories considered here include: univariateML which is, “an R package for maximum likelihood estimation of univariate densities,” which support more than 20 different forms of probability density. kdensity which is, “An R package for kernel density estimation with parametric starts and asymmetric kernels.” This package implements an effectively non-parametric approach to estimating probability densities. overlapping, which is, “a R package for estimating overlapping in empirical distributions.” The obverse process from estimating or fitting probability distributions is arguably drawing samples from defined distributions, of which the humanleague package is an example. This package has a particular application in synthesis of discrete populations, yet the implementation is quite generic and powerful. 4.2.9 Wrapper Packages “Wrapper” packages provide an interface to previously-written software, often in a different computer language to the original implementation. While this category is reasonably unambiguous, there may be instances in which a “wrapper” additionally offers extension beyond original implementations, or in which only a portion of a package’s functionality may be “wrapped.” Rather than internally bundling or wrapping software, a package may also serve as a wrapper thorough providing access to some external interface, such as a web server. Examples of potential wrapper packages include the following: The greta package (with accompanying JOSS article) “for writing statistical models and fitting them by MCMC and optimisation” provides a wrapper around google’s TensorFlow library. It is also clearly a workflow package, aiming to provide a single, unified workflow for generic machine learning processes and analyses. The nse package (with accompanying JOSS paper) which offers “multiple ways to calculate numerical standard errors (NSE) of univariate (or multivariate in some cases) time series,” through providing a unified interface to several other R packages to provide more than 30 NSE estimators. This is an example of a wrapper package which does not wrap either internal code or external interfaces, rather it effectively “wraps” the algorithms of a collection of R packages. Key Considerations: For many wrapper packages it may not be feasible for reviewers (or authors) to evaluate the quality or correctness of the wrapped software, so review could be limited to the interface or added value provided, or the statistical routines within. Wrapper packages include the extent of functionality represented by wrapped code, and the computer language being wrapped. - Internal or External: Does the software internally wrap of bundle previously developed routines, or does it provide a wrapper around some external service? If the latter, what kind of service (web-based, or some other form of remote access)? - Language: For internally-bundled routines, in which computer language e the routines written? And how are they bundled? (For R packages: In ./src? In ./inst? Elsewhere?) - Testing: Does the software test the correctness of the wrapped component? Does it rely on tests of the wrapped component elsewhere? - Unique Advances: What unique advances does the software offer beyond those offered by the (internally or externally) wrapped software? 4.2.10 Categorical Variables Like the category of probability distributions, software for categorical variables also represents an outlier category that nevertheless encapsulates unique software. This category is particularly prominent in software developed to support social sciences, and its inclusion may be justified on that basis alone: The mere inclusion of this category would open up the general process of peer-reviewing of statistical software to much broader areas of the social sciences than would be admissible without this category. Most software in this category generally takes categorical input data and outputs some form of quantitative response or summary statistic. Input data vary from quantitative intervals to discrete orders or ranks to (cross-)tables of categorical frequencies. Most packages seek to derive quantitative estimates from categorical inputs, although the various estimates are of course in no way comparable. It thus seems exceedingly difficult if not impossible to derive general standards for software in this category, even though we currently recommend inclusion because of the unique importance in the social sciences. Examples of software for categorical variables include the following. perccalc is, “An R package for estimating percentiles from categorical variables” hopit is “an R package for analysis of reporting behavior using generalized ordered probit models”. The input data are assumed to be ordered categorical variables typical of survey responses, and the package translates these into estimates of the equivalent continuous latent variables. DscoreApp is, “an user-friendly web application for computing the Implicit Association Test D-score,” where the stated score is a metric of association between categories elucidated in surveys or questionnaires. thurstonianIRT derives a score used to assess forced-choice questionnaires (in which answers must, for example, be either A or B). qsort provides “a new tool for scoring Q-sort Data”, where these data are derived by asking survey respondents to sort categories in some specified order. The input data in this case are thus the sorting orders, rather than the categories themselves. There is also software which takes quantitative input data and outputs discrete categorisations, as exemplified by the multistateutils package, which is a biostatistical package useful for distinguishing and categorising dynamic trajectories (such as disease or treatment pathways). We propose to exclude this as an explicit category, reflecting an assumption that software within this category will generally able to be assessed under other categories listed here. 4.2.11 Networks Network software is a particular area of application of what might often be considered more generic algorithms, as in the example described above of the grapherator package, for which this category is appropriate only because the input data are assumed to represent a particular form of graphical relationship, while most of the algorithms implemented in the package are not necessarily specific to graphs. That package might nevertheless be useful in developing standards because it, “implements a modular approach to benchmark graph generation focusing on undirected, weighted graphs”. This package, and indeed several others developed by its author Jakob Bossek, may be useful in developing benchmarks for comparison of graph or network models and algorithms. Cases of software which might be assessed using such generic graph generators and benchmarks include: mcMST, which is “a toolbox for the multi-criteria minimum spanning tree problem.” gwdegree, which is a package for, “improving interpretation of geometrically-weighted degree estimates in exponential random graph models.” This package essentially generates one key graph statistic from a particular class of input graphs, yet is clearly amenable to benchmarking, as well as measures of stability in response to variable input structures. Network software which is likely more difficult to assess or compare in any general way includes: tcherry is a package for “Learning the structure of tcherry trees,” which themselves are particular ways of representing relationships between categorical data. The package uses maximum likelihood techniques to find the best tcherry tree to represent a given input data set. Although very clearly a form of network software, this package might be considered better described by other categories, and accordingly not directly assessed or assessable under any standards derived for this category. BNLearn is a package “for learning the graphical structure of Bayesian networks.” It is indubitably a network package, yet the domain of application likely renders it incomparable to other network software, and difficult to assess in any standardised way. 4.2.12 Statistical Reporting and Exploratory Data Analysis Many packages aim to simplify and facilitate the reporting of complex statistical results or exploratory summaries of data. Such reporting commonly involves visualisation, and there is direct overlap between this and the Visualisation category (below). This roughly breaks out into software that summarizes and presents raw data, and software that reports complex data derived from statistical routines. However, this break is often not clean, as raw data exploration may involve an algorithmic or modeling step (e.g., projection pursuit.). Examples include: A package rejected by rOpenSci as out-of-scope, gtsummary, which provides, “Presentation-ready data summary and analytic result tables.” Other examples include: The smartEDA package (with accompanying JOSS paper) “for automated exploratory data analysis”. The package, “automatically selects the variables and performs the related descriptive statistics. Moreover, it also analyzes the information value, the weight of evidence, custom tables, summary statistics, and performs graphical techniques for both numeric and categorical variables.” This package is potentially as much a workflow package as it is a statistical reporting package, and illustrates the ambiguity between these two categories. The modeLLtest package (with accompanying JOSS paper) is “An R Package for Unbiased Model Comparison using Cross Validation.” Its main functionality allows different statistical models to be compared, likely implying that this represents a kind of meta package. The insight package (with accompanying JOSS paper provides “a unified interface to access information from model objects in R,” with a strong focus on unified and consistent reporting of statistical results. The arviz software for python (with accompanying JOSS paper provides “a unified library for exploratory analysis of Bayesian models in Python.” The iRF package (with accompanying JOSS paper enables “extracting interactions from random forests”, yet also focusses primarily on enabling interpretation of random forests through reporting on interaction terms. In addition to potential overlap with the Visualisation category, potential standards for Statistical Reporting and Meta-Software are likely to overlap to some degree with the preceding standards for Workflow Software. Checklist items unique to statistical reporting software might include the following: Automation Does the software automate aspects of statistical reporting, or of analysis at some sufficiently “meta”-level (such as variable or model selection), which previously (in a reference implementation) required manual intervention? General Reporting: Does the software report on, or otherwise provide insight into, statistics or important aspects of data or analytic processes which were previously not (directly) accessible using reference implementations? Comparison: Does the software provide or enable standardised comparison of inputs, processes, models, or outputs which could previously (in reference implementations) only be accessed or compared some comparably unstandardised form? Interpretation: Does the software facilitate interpretation of otherwise abstruse processes or statistical results? Exploration: Does the software enable or otherwise guide exploratory stages of a statistical workflow? 4.2.13 Survival Analyses Software for survival analyses obtains its own category here due to the relatively large number of packages. Survival analysis is a unique category only in that it concerns models to process, predict, or analyse time-to-event data. In many other ways software for survival analyses crosses over with many other categories (including the central ML and Bayes/Monte Carlo categories), with aspects presumably covered by standards developed in those respective categories. The multistateutils package, for example, is a biostatistical package useful for distinguishing and categorising dynamic trajectories (such as disease or treatment pathways). Several routines within this package involve assigning these categories to states, and estimating times spent in various states. Although these are survival analyses in a strict sense, that software is likely more appropriate considered under other categories. Perhaps one thing much survival analysis software has in common is similarity of output, at least conceptually, in trying to quantify or predict times to some defined event of a dynamic processes. One package that may potentially be useful in interpreting and comparing the outputs of different survival routines is survxai, which offers “structure-agnostic explanations of survival models,” whether survival neural networks, survival random forests, or any other approach. ccostr “for estimating mean costs with censored data” survPen “for hazard and excess hazard modelling with multidimensional penalized splines”, where hazard is interpreted as time-to-event in comparison with some baseline scenario. GammaGompertzCR which fits “a Gamma-Gompertz survival model to capture-recapture data collected on free-ranging animal populations” (and was described under Bayesian and Monte Carlo software, above). Key Considerations Although survival analysis is a common purpose of statistical software, general methodologies may be too diverse to permit general comparison and standardisation. This category of software nevertheless very generally overlaps strongly with other categories considered here, suggesting that this entire category may be safely excluded from consideration, with the presumption that primary functional of survival software will be addressed by standards devised under the other categories considered here. Proposal This category be excluded from explicit consideration. 4.2.14 Workflow Support “Workflow” software may not implement particular methods or algorithms, but rather support tasks around the statistical process. In many cases, these may be generic tasks that apply across methods. These include: Classes (whether explicit or not) for representing or processing input and output data; Generic interfaces to multiple statistical methods or algorithms; Homogeneous reporting of the results of a variety of methods or algorithms; and Methods to synthesise, visualise, or otherwise collectively report on analytic results. Methods and Algorithms software may only provide a specific interface to a specific method or algorithm, although it may also be more general and offer several of the above “workflow” aspects, and so ambiguity may often arise between these two categories. We note in particular that the “workflow” node in the interactive network diagram mentioned above is very strongly connected to the “machine learning” node, generally reflecting software which attempts to unify varied interfaces to varied platforms for machine learning. Among the numerous examples of software in this category are: The mlr3 package (with accompanying JOSS paper), which provides, “A modern object-oriented machine learning framework in R.” The fmcmc package (with accompanying JOSS paper), which provides a unified framework and workflow for Markov-Chain Monte Carlo analyses. The bayestestR package (with accompanying JOSS paper) for \"describing effects and their uncertainty, existence and significance within the Bayesian framework. While this packages includes its own algorithmic implementations, it is primarily intended to aid general Bayesian workflows through a unified interface. Workflows are also commonly required and developed for specific areas of application, as exemplified by the tabular package (with accompanying JOSS article for “Analysis, Seriation, and visualisation of Archaeological Count Data”. Key Considerations: Workflow packages are popular and add considerable value and efficiency for users. One challenge in evaluating such packages is the importance of API design and potential subjectivity of this. For instance, mlr3 as well as tidymodels have similar uses of providing a common interface to multiple predictive models and tools for automating processes across these models. Similar, multiple packages have different approaches for handling MCMC data. Each package makes different choices in design and has different priorities, which may or may not agree with reviewers’ opinions or applications. Despite such differences, it may be possible to evaluate such packages for internal cohesion, and adherence to a sufficiently clearly stated design goal. Reviewers may be able to evaluate whether the package provides a more unified workflow or interface than other packages - this would require a standard of relative improvement over the field rather than baseline standards. These packages also often contain numerical routines (cross-validation, performance scoring, model comparison), that can be evaluated for correctness or accuracy. 4.2.15 Summary Statistics SmartEDA modelDown insight rsimsum MCMCvis 4.2.16 Spatial Analyses 4.2.17 Education A prominent class of statistical software is educational software designed to teach statistics. Such software many include its own implementations of statistical methods, and frequently include interactive components. Many examples of educational statistical software are listed on the CRAN Task View: Teaching Statistics. This page also clearly indicates the likely strong overlap between education and visualisation software. With specific regard to the educational components of software, the follow checklist items may be relevant. A prominent example is the LearnBayes package. Key Considerations: Correctness of implementation of educational or tutorial software is important. Evaluation of such software extends considerably beyond correctness, with heavy emphasis on documentation, interactive interface, and pedagogical soundness of the software. These areas enter a very different class of standards. It is likely that educational software will very greatly structurally, as interaction may be via graphical or web interfaces, text interaction or some other form. The Journal of Open Source Education accepts both educational software and curricula, and has a peer review system (almost) identical to JOSS. Educational statistical software reviewed by rOpenSci could thus potentially be fast-tracked through JOSE reviews just as current submissions have the opportunity to be fast-tracked through the JOSS review process. Demand: Does the software meet a clear demand otherwise absent from educational material? If so, how? Audience: What is the intended audience or user base? (For example, is the software intended for direct use by students of statistics, or does it provide a tool for educational professionals to use in their own practice?) Algorithms: What are the unique algorithmic processes implemented by the software? In what ways are they easier, simpler, faster, or otherwise better than reference implementations (where such exist)? Interactivity: Is the primary function of the software interactive? If so, is the interactivity primarily graphical (for example, web-based), text-based, or other? 4.3 Proposals Peer review in the system will primarily focus on code written in R, C, and C++. Standards will be written so as to separate language-specific and non-language-specific components with an eye towards further adoption by other groups in the future (in particular groups focussed on the Python language). The system will be limited to R packages, and tools developed will be specific to R package structure, although keeping in mind potential future adaptation and adaptability to non-packaged R code. Standards that may apply to non-packaged are code may also be noted for use in other contexts. Submissions will be required to nominate at least one statistical category, to nominate at least one “reference implementation”, and to explain how the submitted software is superior (along with a possibility to explain why software may be sufficiently unique that there is no reference implementation, and so no claims of superiority can be made). We will only review packages where the primary statistical functionality is in the main source code developed by the authors, and not in an external package. The following 11 categories of statistical software be defined, and be considered in scope: Bayesian and Monte Carlo algorithms Dimensionality Reduction and Feature Selection Machine Learning Regression and Interpolation Probability Distributions Wrapper Packages Networks Exploratory Data Analysis Workflow Software Summary Statistics Spatial Statistics The following categories be considered, at least initially, to be out-of-scope: Educational Software Visualisation Software Beyond these general Proposals, the following lists Proposals specific to particular categories of statistical software: For packages which parameterise or fit probability distributions, develop routines to assess and quantify the sensitivity of outputs to the distributional properties of inputs, and particularly to deviations from assumed distributional properties. We identify a sub-category of software which accepts network inputs, and develop (or adapt) general techniques to generate generic graphs to be used in benchmarking routines. Other software which falls within the category of Network Software only because of restricted aspects such as internal data representations (such as tcherry) not be considered or assessed within that category. References "],
["standards.html", "Chapter 5 Standards 5.1 Other Standards 5.2 Generally Applicable Standards 5.3 Standards Specific to Statistical Software 5.4 Proposals", " Chapter 5 Standards An important output of the present project is a set of standards which can serve as expectations for software and as guides against which developers and reviewers can assess software. Important general questions regarding standards include the following: What kind of standards might apply to software in general? What kind of standards might specifically apply to statistical software? How might such standards differ between different languages? To what extent should we aim for “verification” or “validation” of software, and how might be signify such? We acknowledge that standards of the kind anticipated here will likely be better conceived of to reflect ongoing processes of development. As such, of equal importance to developing a set of standards per se will be developing an understanding of the kinds of processes which may have the most defining effect on resultant standards at any point in time. The remainder of this document employs a convenient distinction between: “General Standards” which may be applied to all software considered within this project, irrespective of how it may be categorized under the times of categories of statistical software listed above; and “Specific Standards” which apply to different degrees to statistical software depending on the software category. It is likely that standards developed under the first category may subsequently be deemed to be genuinely Statistical Standards yet which are applicable across all categories, and it may also be likely that the development of category-specific standards reveals aspects which are common across all categories, and which may subsequently be deemed general standards. We accordingly anticipate a degree of fluidity between these two broad categories. There is also a necessary relationship between the Standards described here, and processes of Assessment described below in Chapter 8. We consider the latter to describe concrete and generally quantitative aspects of post hoc software assessment, while the present Standards provides guides and benchmarks against which to prospectively compare software during development. As this entire document is intended to serve as the defining reference for our Standards, that term may in turn be interpreted to reflect this entire document, with the current section explicitly describing aspects of Standards not covered elsewhere. As described above, we anticipate the ongoing development of this current document to employ a versioning system, with software reviewed and hosted under the system mandated to flag the latest version of these standards to which it complies. 5.1 Other Standards Among the noteworthy instances of software standards which might be adapted for our purposes, and in addition to entries in our Annotated Bibliography, the following are particularly relevant: The Core Infrastructure Initiative’s Best Practices Badge, which is granted to software meeting an extensive list of criteria. This list of criteria provides a singularly useful reference for software standards. The Software Sustainability Institute’s Software Evaulation Guide, in particular their guide to Criteria-based software evaluation, which considers two primary categories of Usability and Sustainability and Maintainability, each of which is divided into numerous sub-categories. The guide identifies numerous concrete criteria for each sub-category, explicitly detailed below in order to provide an example of the kind of standards that might be adapted and developed for application to the present project. The more technical considerations of the Object Management Group’s Automated Source Code CISQ Maintainability Measure (where CISQ refers to the Consortium for IT Software Quality). This guide describes a number of measures which can be automatically extracted and used to quantify the maintainability of source code. None of these measures are not already considered in one or both of the preceding two documents, but the identification of measures particularly amenable to automated assessment provides a particularly useful reference. There is also rOpenSci’s guide on package development, maintenance, and peer review, which provides standards of this type for R packages, primarily within its first chapter. Another notable example is the tidyverse design guide, and the section on Conventions for R Modeling Pacakges which provides guidance for model-fitting APIs. 5.2 Generally Applicable Standards The project aims to establish and maintain a set of standards governing general aspects of software, such as software interfaces, documentation, and testing. The following list represents a synthesis of the preceding sets of reference standards which might be of use to this project, primarily derived from the Software Sustainability Institute’s guide to Criteria-based software evaluation. This list is provided as an example of the kinds of standards considered in other domains, and developed in order to be generally applicable to software. At the present initial stage of this project, we merely present the following list as exemplary, and aim to use it to stimulate and guide discussion regarding the potential utility of adapting, adopting, or otherwise developing an equivalent, and/or equivalently detailed, list of standards. Prior to detailing the exemplary standards adapted from those of the Software Sustainability Institute, we note that both these standards, and influential sources such as Mili (2015), consider software in terms of qualitative aspects such as Usability, Sustainability and Maintainability, rather than in the kinds of concrete standards otherwise commonly considered such as those describing Documentation, Testing, Code Structure, other other aspects of software design. The use of qualitative categories in defining and guiding standards is preferred and adopted here particularly because it facilitates and encourages consideration of those properties as they pertain to multiple aspects of software design, and is likely to better facilitate the design and development of more consistent and unified software. Usability 1.1 Understandability High level description of what/who the software is for High level description of what the software does High level description of how the software works Design rationale - why the software does things the way it does Architectural overview with diagrams Descriptions of intended use cases Case studies of use 1.2 Documentation Provides a high level overview of the software Partitioned into sections for users, user-developers, and developers (depending on the software) Lists resources for further information Is task-oriented Consists of clear, step-by-step instructions Gives examples of what the user can see at each step For problems and error messages, the symptoms and step-by-step solutions are provided Does not use terms like “intuitive”, “user-friendly”, “easy to use”, “simple”, or “obviously” (other than in quotes from satisfied users). States command names, syntax, parameters, error messages exactly as they appear or should be typed. Uses \\({\\tt teletype-style fonts}\\) for command line inputs and outputs, source code fragments, function names, class names, etc. English language descriptions of commands or errors are provided. Plain text files (e.g. READMEs) use indentation and underlining to structure the text. Plain text files do not use TAB characters to indent the text. Documentation is complete (includes configuration requirements or properties). Is held under version control alongside the code Is on the project web site Documentation on web site makes it clear what version of software the documentation applies to. 1.3 Buildability Straightforward to meet build pre-requisites Straightforward to build the software Web site has build instructions Source distributions have build instructions Web site lists all third-party dependencies that are not bundled Source distribution lists all third-party dependencies that are not bundled All mandatory third-party dependencies are currently available All optional third-party dependencies are currently available 1.4 Installability Web site has installation instructions Binary distributions have installation instructions Web site lists all third-party dependencies that are not bundled 1.5 Learnability A getting started guide is provided with a basic example Instructions are provided for many basic use cases Reference guides are provided for all options Documentation is provided for user-developers and developers 1.5 Performance Sustainability &amp; Maintainability 2.1 Identity Identity of project is clear and unique both within domain of application and generally. Project/software has own domain name Project/software has distinct name within application area (appears within first page of search results when entered with domain keywords). Project/software has distinct name regardless of application area Project/software does not throw up embarrassing “Did you mean …” suggestions on search engines. Project/software name does not violate a trademark Project/software name is trademarked 2.2 Copyright Web site states copyright Web sites states developers and funders If multiple web sites, then all state exactly same copyright, license, authorship Each source code file has copyright statement If supported by language, each source file has copyright statement embedded within a constant - Each source code file has a license header 2.3 Licencing Appropriate licence Web site states licence Software has a licence Software has an open source licence Software has an Open Software Institute recognised licence 2.4 Governance Management is transparent Project has a defined governance policy Governance policy is publicly available 2.5 Community To what extent does an active user community exist? Web site has statement of numbers of users/developers/members Web site has quotes from satisfied users Web site lists most important partners or collaborators Web site has list of project publications Web site lists third-party publications that use the software Web site lists software that uses/bundles this software Users are required to cite software if publishing results derived from its use Users exists who are not members of the project Developers exists who are not members of the project 2.6 Accessibility To what extent is software accessible? Binary distributions are available Binary distributions are available without need for registration or authorisation Source distributions are available Source distributions are available without need for registration or authorisation Access to source code repository is available (whether for free, payment, registration) Anonymous read-only access to source code repository Ability to browse source code repository online Repository hosted in sustainable third-party site which will live beyond lifetime of any current funding Downloads page shows evidence of regular releases 2.7 Testability Straightforward to test software to verify modifications Project has unit tests Project has integration tests Project has scripts for testing non-automated scenarios (e.g. GUIs) Project recommends tools to check conformance to coding standards Project has automated tests to check conformance to coding standards Project recommends tools to check test coverage Project has automated tests to check test coverage A minimum test coverage level has been defined There is an automated test for this minimum level Tests are automatically run nightly Continuous integration is supported Test results are visible to all developers/members Test results are visible publicly Project specifies how to set up external resources (FTP servers, databases, etc.) for tests Tests create their own files, database tables, etc. 2.8 Portability To what extent can software be used on other platforms? (Checkboxes for various platforms.) 2.9 Supportability To what extent will software be supported currently and in the future? Web site has page describing how to get support User doc has page describing how to get support Software describes how to get support (in README) Project has an e-mail address Project e-mail address has domain name E-mails are read by more than one person E-mails are archived E-mails archives are publicly readable E-mail archives are searchable Project has a ticketing system Ticketing system is publicly available Ticketing system is searchable Web site has a site map or index Web site has a search facility Project resources are hosted externally in a sustainable third-part repository which will live beyond lifetime of current project E-mail archives or ticketing system shows that queries are resounded to (not necessarily fixed) within a week. If there is a blog, it is regularly used E-mail lists of forums, if present, have regular posts 2.10 Analysability Source code is structured into modules or packages Source code structure relates clearly to the architecture or design. Source code repository is in a version control system Structure of source code repository and how this maps to software’s components is documented Source releases are snapshots of the repository Source code is commented Source code comments are written in a document generation mark-up language Source code is laid out and indented well Source code uses sensible class, package, and variable names There are no old or obsolete source code files that should be handled by version control There is no commented out code There are not TODOs in the code Auto-generated source code is in separate directories from other source code Regeneration of auto-generated source code is documented Coding standards are recommended by the project Coding standards are required to be observed Project-specific coding standards are consistent with community standards 2.11 Changeability Project has a defined contributions policy Contributions policy is publicly available Contributors retain copyright/IP of their contributions Users, developer members, and developers who are not members can contribute Project has a defined and stable deprecation policy Stability/deprecation policy is publicly available Releases document deprecated components within release Releases document removed or changed components within release 2.12 Evolvability Web site describes project roadmap, plans, or milestones Web site describes how project is funded or sustained Web site describes end date of current funding lines 2.13 Interoperability Uses open standards Uses mature, ratified, non-draft standards Provides tests demonstrating compliance with standards In contrast to these qualitative aspects, Mili (2015) identifies the following attributes of Software Quality: Functional Attributes 1.1 Correctness 1.2 Robustness Useability Attributes 2.1 Ease of Use 2.2 Ease of Learning 2.3 Customizability 2.4 Calibrability 2.5 Interoperability Structural Attributes 3.1 Design Integrity 3.2 Modularity (including “cohesion” and “coupling”) 3.3 Testability 3.4 Adaptability Other aspects derived from other forms of standards include: Must use https Must be in English Unique version numbering Semantic versioning Release notes Static code analysis (with links to lots of language-specific tools) Dynamic code analysis (with links to lots of language-specific tools) Use of memory check tools, or memory-safe languages Functions that are type stable unless explicitly indicated and explained otherwise. (See also the section on type stability in the tidyverse design guide.) An additional area for consideration is the creation of tools for documentation creation and evaluation based on metadata of statistical method inputs and outputs and packaged data (Lenhardt et al. 2014). Relationships between data and statistical software may be structured in a sufficiently systematic way to permit systematic documentation. 5.2.1 Testing Testing is a critical area for standards, as tests are a concrete manifestation of standards and the means by which authors may demonstrate compliance. While testing is considered best practice and test coverage often used as a measure of test completeness, guidance on what to test is rare, especially in the context of R packages. Thus, standards will need to provide guidance on the types and methods of tests required for different statistical software categories. (The “Turing Way” has a useful discussion of different kinds of software tests.) In addition, statistical software may benefit from means or modes of testing beyond the common frameworks used in and for R packages (e.g. R RMD check, testhtat). A variety of other frameworks and workflows from other languages and contexts may be relevant. Almost all testing as currently implemented in R is “concrete testing” (Mili 2015), and little consideration has been given in R to “stochastic” or “property-based” testing, in which expectation values of inputs and outputs are tested, rather than concrete instantiations of such (the notably exception of the apparently abandoned fuzzr package notwithstanding). Other languages have developed grammars for stochastic or property-based testing, notably through the hypothesis package for python. These grammars enable specification of test assumptions as well as expected test outputs. Assumptions in hypothesis are declared through simple @given statements that might, for example, quantify an assumed probability distribution for input data, while outputs are specified through equivalent @expect statements that might, for example, specify expected distributional properties of an output rather than just concrete values. The following are likely key questions which we will need to address regarding testing: To what extent should testing focus on functional or integration rather than unit testing? Is it sufficient to consider test execution as an integral part of R CMD check only? Or might there by a case for developing alternative test execution environments and approaches? For instance, should there be an alternate workflow for long-running tests, tests requiring large data, or tests intended to be executed for other purposes? Is it worthwhile concretely defining one or more goals of testing? (Such as error detection, error frequencies, error tolerance, accuracy.) What are the test data? And how easy is it to input alternative data to tests? Is there scope for “stochastic” or “property-based” testing? What test reporter should be used? Does the testthat package and similar suffice? Or might it be worth considering new test reporting systems? What aspects of tests and test data (both actual and permissible) might be worthwhile documenting in some kind of metadata format? Extant R package which address some of these issues include tinytest, roxytest, and xpectr. 5.2.2 Documentation Standards will include requirements for form and completeness of documentation. As with interface, several sources already provide starting points for reasonable documentation. Some documentation requirements will be specific to the statistical context. For instance, it is likely we will have requirements for referencing appropriate literature or references for theoretical support of implementations. Another area of importance is correctness and clarity of definitions of statistical quantities produced by the software, e.g., the definition of null hypotheses or confidence intervals. Data included in software – that used in examples or tests – will also have documentation requirements. It is worth noting that the roxygen system for documenting R packages is readily extensible, as exemplified through the roxytest package for specifying tests in-line. 5.3 Standards Specific to Statistical Software The applicability of any concrete set of standards is likely to differ between different categories of statistical software. For example, metrics of numerical accuracy will likely differ between categories primarily describing analytical algorithms and those describing less tractable routines which produce less directly reproducible results. Or consider metrics derived from tests, which must be interpreted in qualitatively different ways for packages entirely dependent on their own internal code versus packages largely dependent on the results of calls to external data providers (along with additional differences between, for example, locally-installed “external” providers versus online sources of external data). Different standards must thus be considered to be differentially applicable to different categories of software, and thus the interplay between the scope of statistical software considered above and throughout this project, and the standards emerging from the project, will be of critical importance throughout the project. Such considerations lead to the following kinds of questions which will likely have to be addressed: To what extent ought we aim for general standards at the expense of specific abilities to assess particular categories of statistical software? To what extent ought we strive for automation of software assessment, given the inherent risk of overseeing qualitative differences between different categories? How much effort should be expended both developing a categorization of statistical software, and understanding the potential effects of such a categorization? The following exemplify a few categories of statistical standards which may be considered, emphasising restrictions of applicability to alternative kinds of software. Numerical standards such as precision or convergence. These will be applicable only to some restricted subset of all potential categories of statistical software (likely including but not limited to analytic and, to some extent, predictive routines) Moreover, even these two categories alone will likely require differing standards for precision or convergence. Method validity It may be necessary or useful to develop standards for the validity of a chosen method, independent of its implementation. Questions of validity are commonly related to domains of application, and therefore must relate directly to any system for categorising statistical software. A method may (have been demonstrated to) be valid for some particular domain of application, and a software routine may be developed to adapt that method to some previously untried domain. It may then be necessary to consider potential (in)validity of that software, along with potential validity in other domains, themselves potentially not explicitly considered by the software authors. Software scope The preceding considerations extend directly to general concerns of scope, whether in terms of domains of applicability, properties of input or output data, authorial intentions, or other contextual factors. Scope in all of these senses obviously must directly affect and determine the kinds of standards which may or may not apply to software, just as defining scope in these senses is also effectively an exercise in categorization of the kind described above. Reference standards For software which implements or relies on standard routines, it may be necessary to designate reference data or implementations against which to compare outcomes, or guidance in selecting such references. For instance, the National Institute of Standards and Technology of the U.S. provides a collection of reference data sets with certified computational results which statistical software should be able to reproduce. The project may benefit from collaboration with the ongoing development of the Transparent Statistics Guidelines, by the “HCI (Human Computer Interaction) Working Group”. While currently only in its beginning phases, that document aims to provide concrete guidance on “transparent statistical communication.” If its development continues, it is likely to provide useful guidelines on best practices for how statistical software produces and reports results. Specific standards for neural network algorithms have been developed as part of a google 2019 Summer Of Code project, resulting in a dedicated R package, NNbenchmark, and accompanying results—their so-called “notebooks”—of applying their benchmarks to a suite of neural network packages. We envision the present project proceeding from this initial stage by developing parallel definitions for both categories of software (defining both in-scope and beyond-scope), and specific standards. A simple way to proceed may be to develop lists for both, along with a representation of inter-connections between categories and standards. 5.3.1 Demonstration of innovation, novelty, or advancement The standards listed above largely refer to minimum requirements for software, but following common practice in academic publishing, one may also possibly require that a piece of software is demonstrably superior to otherwise equivalent software in at least one (or perhaps more?) specific way(s). (In current rOpenSci standards, packages are required to demonstrate signicant improvement over similar packages). If such a “relative improvement” requirement is included in the process, authors may be required to demonstrate how their software exceeds existing or reference implementations of similar tools in some of the following ways: Efficiency: Is the software more efficient (faster, simpler, other interpretations of “efficient”) than reference implementations? Reproducibility or Reliability: Does the software reproduce sufficiently similar results more frequently than reference implementations (or otherwise satisfy similar interpretations of reproducibility)? Accuracy or Precision: Is the software demonstrably more accurate or precise than reference implementations (such as ? Simplicity of Use: Is the software simpler to use than reference implementations? Algorithmic Characteristics: Does the algorithmic implementation offer characteristics (such as greater simplicity or sensitivity) superior to reference implementations? If so, which? Convergence: Does the software provide faster or otherwise better convergence properties than reference implementations? Method Validity: Does the software overcome demonstrable flaws in previous (reference) implementations? If so, how? Method Applicability: Does the software enable a statistical method to be applied to a domain in which such application was not previously possible? Automation Does the software automate aspects of statistical analyses which previously (in a reference implementation) required manual intervention? Input Data: Does the software “open up” a method to input data previously unable to be treated by a particular algorithm or method? Output Data: Does the software provide output in forms previously unavailable by reference implementations? Reference Standards: Are there any reference standards, such as the US National Institute of Standards and Technology’s collection of reference data sets against which the software may be compared? If so, which? 5.4 Proposals We develop a concrete list of standards like those of the Software Sustainability Institute given above. We identify which items in the resultant list are amenable to automatic assessment, and implement procedures to automate such assessment as far as practicable. Our standards will be versioned, and software will be aligned with the most recent version of these standards with which it complies. We initially develop a minimal set of standards applicable to different categories of statistical software, and aim to develop such category-specific standards throughout the ongoing development of the project. References "],
["assessment.html", "Chapter 6 Assessment 6.1 General Software Metrics 6.2 Metrics specific to statistical software 6.3 Diagnostics and Reporting 6.4 Proposals and Aims", " Chapter 6 Assessment The preceding Standards section primarily served (in its current form) to illustrate one possible approach to standards used to prospectively guide software during development. In contrast, the present section lists aspects of software, both general and specific to statistical software, which may usefully assessed in order to give insight into structure, function, and other aspects. The following list of assessments is ultimately intended to inform our development of a stand-alone tool, and potentially also a publicly available service, which can be used by any developers to assess their own software. We accordingly intend the following standards to be used both for retrospective purposes of peer review, and for prospective use in developing software both in general, and in preparation for peer-review. It is important to consider the applicability of each metric to different categories of statistical software, as well as the extent to which the following aspects may be more or less applicable or relevant at different phases of a software life cycle, or how expected values for, or results of applying, metrics may vary throughout a software life cycle. 6.1 General Software Metrics The following is an incomplete list of the kinds of metrics commonly used to evaluate software in general, and which might provide useful for assessing statistical software in the present project. Code structure Cyclomatic complexity Codebase size Function size / number Numbers of external calls within functions Numbers and proportions of Exported / non exported functions Code consistency Dynamic metrics derived from function call networks or similar Network-based metrics both for entire packages, for individual functions, and derived from analyses of test coverage Functional overlap with other packages Documentation metrics: Numbers of documentation lines per function Proportion of documentation to code lines Presence of examples Vignettes Data documentation metrics Intended and/or permitted kinds of input data Nature of output data Description of data used in tests Meta structure Dependencies Reverse dependencies Meta metrics License (type, availability, compatibility) Version control? Availability of website Availability of source code (beyond CRAN or similar) Community: Software downloads and usage statistics Numbers of active contributors Numbers or rates of issues reported Maintenance: Rate/Numbers of releases Rate of response to reported issues Last commit Commit rate stars (for github, gitlab, or equivalent for other platforms) forks Extent of testing Code coverage Examples and their coverage Range of inputs tested Nature of testing Testing beyond R CMD check? Testing beyond concrete testing? 6.2 Metrics specific to statistical software Metrics specific to statistical software will depend on, and vary in applicability or relevance with, the system for categorizing statistical software expected to emerge from the initial phase of this project. Details of this sub-section will be largely deferred until we have a clearer view of what categories might best be considered, which we are hopeful will emerge following the first committee meeting, and in response to ensuing feedback. In the meantime, metrics can be anticipated by referring to the preceding examples for categories of statistical software (numerical standards, method validity, software scope, and reference standards). We anticipate having a number of such categories, along with a number of corresponding metrics for assessing software in regard to each category. As mentioned at the outset, software will generally be expected to fit within multiple categories, and specific metrics will need to be developed to ensure validity for software encompassing any potential combination of categories. 6.3 Diagnostics and Reporting While the preceding sub-sections considered what might be assessed in relation to statistical software, the project will also need to explicitly consider how any resultant assessment might best be presented and reported upon. Indeed, a key output of the project is expected to be a suite of tools which can be used both in this and other projects to construct, curate, and report upon a suite of peer-reviewed software. Moreover, we will aim to develop these tools partly to provide or enhance the automation of associated processes, aiming both to enhance adaptability and transferability, and to ensure the scalability of our own project. It is useful in this context to distinguish between collective tools useful for, of applicable to, collections of software, of individuals, or of processes pertaining to either (here, primarily peer review), and singular tools of direct applicability to individual pieces of software. We envision needing to address the (likely relative) importance of some of the following kinds of diagnostic and reporting tools which may be usefully developed. Collective Tools Qualitative tools useful in assessing or formalizing categories of software Quantitative tools to retrospectively assess such aspects as: Collective “quality” of software Community engagement Effectiveness (or other metrics) of review Singular Tools Quantitative tools that can be prospectively used to Improve or assure software quality Document aspects of software quality Aid modularity or transferability either of software, or of the tools themselves Tools to formalize structural aspects of software such as tests (for example, through implementing new frameworks or grammars) Extensions of extant packages such as lintr, covr, goodpractice Comparisons of package metrics to distributions for other packages or systems (such as the CRAN archive directories) Diagnostic and report aggregation, design, or automatic creation at any stage before, during, or after peer review. The one question of abiding importance is the extent to which any such tools, and/or the automation of processes which they may enable, might enhance any of the following aspects: Software development Peer review of software Wider communities of users or developers The adaptation of our system to other domains 6.4 Proposals and Aims We develop a system to automatically assess aspects of software detailed in the above list (and more).AIM: To assess software in as much detail as possible, while reducing the burden on developers of manual assessment. The development of this system extend from the riskmetric package developed by the PharmaR group.AIM: To efficiently re-use work, prevent duplication, and develop our system as quickly as possible. We provide this assessment both as a stand-alone R package, and a publicly available service which receives text links to publicly available repositories (and does not enable upload of binary software).AIM: To extend engagement with the present project beyond just those directly interested in submitting software for review, and so to enhance broader community engagement with both this project, and rOpenSci in general. Considerable effort be devoted to developing a coherent and adaptable system to summarise and report on software assessment.AIM: To devise a reporting system which imposes a low cognitive burden on both developers and reviewers needing to assess packages. We simultaneously develop a system for assessing historical development of all available metrics, through applying our assessment system to all packages in the CRAN and bioconductor archives, and we use comparisons with these historical patterns to inform and guide our reporting system.AIM: To ensure standards and assessments reflect enacted practices, rather than just theoretical assumptions, and to ensure that standards and assessments keep apace with ongoing developments of practices. "],
["lifeycle.html", "Chapter 7 Software Review and Life Cycle Models [Seeking Feedback] 7.1 Other systems for software and peer review 7.2 Software Life Cycle Considerations", " Chapter 7 Software Review and Life Cycle Models [Seeking Feedback] Prior to describing the review system we intend to develop, we briefly digress to describe analogous review systems, aiming to emphasise aspects which may be useful to adopt within our system. This chapter concludes with succinct proposals of aspects of these prior systems which we intend to adopt within our system, and for which we are explicitly seeking feedback. There are notable differences between the systems described here, with contrasts between them often providing convenient reference points in considering many of the subsequent review phases we envision developing. The present chapter then concludes with brief consideration of what a model of the life cycle of open source software might look like. This is important in guiding the structure of the proposed review process. 7.1 Other systems for software and peer review 7.1.1 rOpenSci rOpenSci’s current software peer-review process, detailed in our developer guide, is based on a blend of practices from peer review of academic practices and code review in open-source projects. Review takes place via an issue thread in our “software-review” repository on GitHub. The review process is entirely open, with each issue thread used to manage the entire process, coordinated by rOpenSci’s editors. After initial screening for scope and minimal qualification by editors, two reviewers provide comments and feedback on software packages. After one or more rounds of revisions, packages reach a point of approval, at which point they are “accepted” by rOpenSci, symbolized both through a badge system, and (generally) through transferring the software from the authors’ private domain to the github.com/ropensci domain. 7.1.2 The Journal of Open Source Software The Journal of Open Source Software (JOSS) was based on rOpenSci and follows a similar approach, with greater automation and broader scope. The Journal of Statistical Software conducts a closed review of both manuscript and software, with fewer prescriptive standards. In reviewing packages for acceptance into its repository, BioConductor conducts an open review primarily aimed at maintaining minimum standards and inter-compatibility. 7.1.3 Academic Journal Reviews One ubiquitous model for processes of peer review is that of “standard” academic journals, for which we now highlight two relevant aspects. 7.1.3.1 Primary and Secondary Editors Academic journals commonly have a board of (primary) editors, and a field of (secondary) subject or specialist editors. The initial and terminal points of review processes are commonly handled by the primary editors, who delegate subject editors to both solicit appropriate reviewers, and to manage the review process. Upon completion, the primary editor generally signifies ultimate acceptance. Such a model would also likely be beneficial for the present project, in spite of potential difficulties we may face in attracting sufficient numbers of subject editors. Division of labour between primary and secondary editors would offer distinct advantages, foremost among which would be an ability to appoint a reasonably large number of “subject editors” (or equivalent), for whom such an official designation would be able to be used to boost their own careers. As a contrast, rOpenSci’s editorial processes are handled by a single cohort of eight editors, of whom four are staff members, while JOSS currently has six primary editors and 31 “topic” editors. The preceding consideration of categories suggests we may end up with around a dozen categories, and so potentially be able to offer around this number (or more) of honorary subject editor positions, along with a concomitant reduction in workload for each of these. Engaging such a range of subject editors would also lessen the burden on primary editors, perhaps enabling the system to be initially trialled with the two or three people primarily engaged in its current developmental phase. The engagement of a wider range of subject editors would also enlarge the network of people directly engaged with the project, as well as extending its sphere of influence to encompass the professional networks of all those involved. 7.1.3.2 Invited and Mentored Submissions Many journals enable editors to personally invite selected authors to submit manuscripts on some particular topic, often compiled within single “special issues”. While special issues may not be relevant here, the notion of invited submissions may prove particularly useful in fostering integration between software packages. One likely defining distinction between rOpenSci and RStudio may be the ability of the latter organisation to strategically plan the development of software that links pre-existing software into more coherent or thematically-aligned “suites” of software (the tidyverse likely being the prime example). In contrast, rOpenSci’s software profile is very largely dependent on the whims of largely independent developers, and the software they autonomously elect to submit. (rOpenSci staff may themselves also develop software, and so strive to create more focussed suites of related packages, but this is then by definition more an individual than community effort.) The ability to solicit software within particular categories, or fulfilling particular functionality, may greatly aid an ability for this project to develop a coherent and singularly identifiable suite of packages for use in statistical analyses. One potential ways by which submissions could be invited would be through all regular meetings of the editors and board having a fixed discussion point on potential categories in which submissions may be desired. Agreement on the importance or usefulness of particular categories or themes may be relatively rare, but having this as a fixed item would allow progressive contemplation ultimately leading to sporadic consensus. Following meetings during which such consensus emerges, a general call for themed submissions may be issued, and/or specific potential package authors may be individually approached. The solicitation of themed submissions may also involve editors, members of the board, or other community members, offering their services as mentors or advisors throughout processes of package development. Invited submissions would then also serve as an opportunity for dissemination of the knowledge and expertise built up and held by individuals prior to and throughout the life of this project. Extending on from that idea, it may be worthwhile examining a “mentorship” system, whereby people who might feel they lack the skills necessary to develop a package of suitable standards might apply via an alternative form of pre-submission enquiry (in this case something more like a “pre-development enquiry”) as to whether anybody might be willing to mentor the development of a particular package idea. Such a system would of course require individuals to be willing to volunteer their services as mentors, but would have potentially significant advantages in expanding the entire system well beyond the boundaries of the limited few who have sufficient confidence in their abilities to develop packages. Proposal We adopt a model of primary and secondary editors, through having the rOpenSci staff directly involved in the development of this project serve as primary editors, and we seek to find and nominate subject editors as soon as we have reached agreement on categories of statistical software. Members of the board may also offer their services in either as primary or secondary editorial capacity. Once the system has started, we implement a fixed discussion point of every meeting on potential themes for invited submissions, and sporadically issue open (and directed) invitations for submissions of category-specific software. We offer a separate stream of “pre-development enquiry” as a kind of “ideas lab” to which people may submit and discuss ideas, with the system explicitly designed to connect ideas to potential mentors who may guide development towards full packages. 7.1.4 The Debian System The development of software for the open-source Debian Operating System is guided by Debian Developers and Debian Maintainers. Expressed roughly, maintainers are individuals responsible for the maintenance of particular pieces of software, while developers engage with activities supporting the development of the operating system as a whole. The submission and review process for Debian is almost entirely automated, based on tools such as their own software checker, lintian. Debian differs fundamentally from the system proposed here in being centred around the trust and verification of people rather than software. Submission of software to Debian is largely automatic, and bug-free software may often progress automatically through various stages towards acceptance. Software may, however, only be submitted by official Debian Maintainers or Developers. People can only become developers or maintainers through being sponsored by existing members, and are then subject to review of the potential contribution they may be able to make to the broader Debian community. (Details can be seen in this chapter of the Debian handbook.) While the general process for software submission and acceptance in Debian may not be of direct relevance, their versioning policy may provide a useful mode. The ongoing development of both the Debian system and all associated packages proceeds in accordance with a versioned policy manual. All new packages must comply to the current standards at the time of submission, and are labelled with the latest version of the standards to which they comply, noting that, For a package to have an old Standards-Version value is not itself a bug … It just means that no-one has yet reviewed the package with changes to the standards in mind. Each new version of the standards is accompanied by a simple checklist of differences, explicitly indicating differences with and divergences from previous versions. As long as software continues to pass all tests, upgrading to current standards remains optional. Failing tests in response to any upgrading of standards serve as a trigger for review of software. The nominated standards version may only be updated once review has confirmed compliance with current standards. We propose to adapt some of these aspects of the Debian system in the present project, as described below. 7.1.5 Other Potential Models The Linux Core Infrastructure Initiative provides badges to projects meeting development best practices. Badges are graded (passing/silver/gold), and awarded by package authors self-certifying that they have implemented items on a checklist. 7.2 Software Life Cycle Considerations The importance of considering Software “life cycles” has long been recognized for closed-source proprietary software, yet life cycles have only been given scant consideration in contexts of open source software (exceptions include Stokes 2012; Lenhardt et al. 2014). A long history and tradition in both practice and published literature on software review (for example, Mili 2015; Ammann and Offutt 2017) generally concludes that software review is most effective when it is an ongoing process that is structurally embedded within a software life cycle, and when review occurs as frequently as possible. Such practices contrast strongly with the singular nature of review as currently implemented by rOpenSci. An effective system for peer review of statistical software is thus may lie somewhere between the current “one-off” practices of rOpenSci and the JOSS, and frequent, ongoing review typical of software development in active teams. An analysis of the effects of rOpenSci’s review process on a few metrics of software development activity revealed that software development tends to stagnate following review. This may be interpreted to reflect software having reached a sufficiently stable state requiring relatively little ongoing maintenance. However, we note that metrics of community engagement with software are generally positively related to the metrics of development activity considered there. Slowing of software development following review may also accordingly reflect or result in decreases in community engagement. Potential systems to enhance review of the kind current practiced by rOpenSci, and particularly to encourage and enable more ongoing review on smaller scales and shorter time frames—and ultimately to encourage the ongoing and active development of software following review—include pull-request reviews, and systems for providing inline code reviews (such as watson-ruby). In addition, ongoing “review” may be explicit in considering the role of user feedback, for instance, in defining and updating the scope of statistical routines (see “Standards for Statistical Software” below). References "],
["process.html", "Chapter 8 The Review Process [SEEKING FEEDBACK] 8.1 Self-Evaluation of Software Prior to Submission 8.2 Pre-Submission Communication 8.3 Reviewers / Selection 8.4 Submission 8.5 Initial Screening 8.6 Review Process 8.7 Acceptance / Scoring / Badging 8.8 Post-acceptance Dissemination, Publication, etc. 8.9 Ongoing Maintenance 8.10 Structured Review beyond Acceptance", " Chapter 8 The Review Process [SEEKING FEEDBACK] This section attempts to briefly describe the entire workflow envisioned to emerge from this project, from tools to enable authors to self-assess packages prior to submission, to tools for identifying and assigning reviewers, to methods and tools for structured review interventions after software has been officially accepted. Although there are several potential general models we could adapt for our proposed system, we anticipate the general workflow being based on github, for which two of the most prominent current models are rOpenSci’s own submission process, and that of the Journal of Open Source Software (JOSS). Both of these systems treat submissions as issues within dedicated review repositories, an approach we intend to adopt. 8.1 Self-Evaluation of Software Prior to Submission A strong focus of this project will be the development of tools to assess software, both generally and for statistical software specifically. One important aim is to develop tools able to be used by software authors to assess their own software. Such self-assessment, along with associated standardised reporting of results, will ease pre-submission enquiries both on the part of submitting authors, and editors responsible for assessing such enquiries. Standardised reporting is considered in the submission phase, while the remainder of the present sub-section considers tools for self-assessment. Current rOpenSci practices expect authors to assess their software using our package standards, then editors perform automated assessment using goodpractice, a package which runs R CMD check as well as other tests including calculating test coverage, linting, and checking for some common coding anti-patterns. The PharmaR project’s riskmetric package performs similar functions as well as providing more development-based metrics and providing a more extensible framework. While authors commonly use the goodpractice assessment tool, demonstrated self-assessment is not currently required at submission. We anticipate developing a system for self-evaluation of software, both in generic form able to be widely applied to software regardless of category, as well as specific tools for statistical software. Many of these generic assessments have been listed at the end of the preceding Framework, while tools specific to statistical software have been considered in the preceding Scope chapter. Key Considerations The primary consideration here is actually one of the primary considerations of the entire project, which is what sort of tools might best be developed? It will be possible to develop extremely sophisticated tools, but at the expense of compromising progress in other important aspects of the project. Perhaps more than any other aspect of this project, answering this question will require maintaining a keen awareness of the compromises necessary to successfully deliver all desired project outcomes. Proposal Authors will be expected to run automated self-assessments prior to submission. We develop a tool for general assessment of software and reporting of analytics, with several modules extending to specific assessment of statistical software. We simultaneously develop a lightweight infrastructure to enable such assessment and reporting to be provided as an online service, so that authors can run assessment in the same environment as it will be run at submission. 8.2 Pre-Submission Communication Pre-Submission Communication is the stage currently practised by both rOpenSci and JOSS whereby authors can enquire as to whether a potential submission is likely to be considered within scope prior to full submission. Full submissions can be potentially onerous, and the pre-submission phase represents a considerable easing of the burden on authors through enabling them to ascertain the potential suitability of a submission prior to completing a full submission. For this reason, we intend to adopt and adapt this phase as part of the new peer-review system. rOpenSci has github issue templates both for pre-submissions and submissions, whereas the both pre-submission and submission enquiries to JOSS are initiated through an external (non-github) website which automatically opens an issue on github with initial details provided by the submitting author. These two systems have two major differences: rOpenSci’s pre-submission enquiries are entirely optional, whereas initial submissions to JOSS are by default pre-submission enquiries (unless originating elsewhere, such as from a completed rOpenSci review). Pre-submission enquiries to rOpenSci serve the singular purposes of determining suitability of a potential full submission, whereas those to JOSS serve the additional purpose of seeking and assigning reviewers. Having found both editors and reviewers, a simple bot command of @whedon start review suffices to automatically transform the pre-submission to a full submission (as a new issue). JOSS is also trialling the automatic generation and reporting of initial software metrics in response to pre-submission enquiries, as in this example. The generation is triggered by the command @whedon check repository, and currently generates a CLOC (Count Lines of Code) summary of lines devoted to various computer languages, and a contribution chart with commits, additions, and deletions from each contributor to the repository. The CLOC output is used to automatically add labels to the issue identifying the primary computer languages. Determining whether or not a potential submission lies within or beyond scope requires aligning software with the statistical categories described above. The processing of pre-submission enquiries is accordingly also expected to entail the categorisation of software. While it may be possible to automate some aspects of software categorisation, we do not currently envision such automated tools being developed during the initial stages of this project. Proposals All submissions be initiated as mandatory pre-submission enquiries which may be automatically transitioned to full submissions upon successful nomination of editors and reviewers, as for JOSS. This has the distinct advantage of separating the search for reviewers from the actual review process itself, leaving resultant review issues notably cleaner and more focussed. Mandatory pre-submission enquiries also add clarity through removing potential ambiguity in deciding between two distinct ways to commence submission. The process of pre-submission be partially automated in a similar manner to the current system of JOSS, with metrics extended and adapted to the unique needs of our own project. Only cursory metrics pertinent to the pre-submission stage will be generated, as exemplified by the JOSS system of using CLOC output to assign labels identifying primary computer languages. Submitting authors be requested to identify potential categories describing their software as part of a pre-submission enquiry, with final categorisation being determined through mutual agreement between editors and submitting authors. Automation procedures may perhaps be extended by some form of automated identification or suggestion of appropriate reviewers, with some aspects of the processes described in the following section potentially automatically triggered by a pre-submission enquiry. Questions Which software metrics might aid the pre-submission process? 8.3 Reviewers / Selection The solicitation of reviewers is one of the most difficult tasks facing any peer review system, including both rOpenSci and JOSS. rOpenSci has built up an extensive network of users, participants, and developers, many of whom are members of the organisation’s slack group. In contrast to traditional academic journals, submitting authors are not requested to recommend potential reviewers. JOSS explicitly states, if you have any suggestions for potential reviewers then please mention them here in this thread, and also points authors of pre-submission enquiries to a curated list of potential reviewers (as a google document), further requesting that authors suggest any potentially appropriate reviewers from that list. Being a google document, it is simply progressively extended line-by-line, currently amounting to 1,163 names, likely making it not particularly easy for authors to find appropriate reviewers. We will of course extend upon our existing pool of reviewers, and also intend to cultivate and extend a network of reviewers with expertise in statistical software throughout the duration of this project. We would also very much like to develop and utilise tools which may aid the process of finding and soliciting reviewers, with the remainder of this sub-section exploring a few options. 8.3.1 Database of Potential Reviewers As described, JOSS maintains a database of potential reviewers within a publicly accessible google document, while rOpenSci maintains theirs in an private airtable. The debian system also maintains a comprehensive database of developers, maintainers, and other stakeholders, publicly accessible for search and via password for restricted access. Private directories have the advantage of allowing for including notes such as review quality and timeliness that may not, but also thus need to be more aggressively managed under standards such as GPDR. Different database platforms also have different privacy advantages. 8.3.2 Automating the Identification of Potential Reviewers It may be worthwhile developing automatic tools to aid identifying appropriate reviewers. One possibility may be analyses of all openly-available code from potential reviewers, to somehow measure degree of similarity with a given submission. While this would be almost impossible to do between different computer languages, it may be possible within R code alone, through processing output from the utils::getParseData function to identify frequencies of usage of various function calls. Such an approach may have important advantages, notably in highlighting reviewers for reasons other than mere prominence within some form of public arena. Appropriate development of such a tool should ultimately enable and empower a more equitable system which is actively designed to avoid any tendency of submitting authors suggesting similar names of centrally prominent developers. 8.4 Submission Submission is envisioned to mirror rOpenSci’s current submission process to a certain degree, although we anticipate a more extensive and structured checklist (or equivalent) system, along with the development of automated tools triggered in response to submission. For example, the current rOpenSci system requires editors run diagnostics locally and to paste the goodpractice output after submission. Such a process is readily automated (as exemplified by JOSS’s current experimental system), and we expect to extend and refine both the automated checking described above, and to collate results within some kind of reporting system. The self-identification of appropriate categories may also trigger automated checking using software specific to various categories of statistical software, with associated output also being automatically inserted into an issue. For both JOSS and rOpenSci’s, current submissions occur via GitHub Issue template, which is primarily a checklist of broad or general attributes with which both software and associated online repositories are expected to comply. Submissions to JOSS have a more extensive and detailed template, which is filled out after initial submission form. We may explore submission via a other mechanisms, forms that automatically generate templates, or an R-based workflows similar to devtools::release(). Key Considerations Presuming the primary entry point to be via pre-submission enquiries as described above means that considerably more information will be available upon transitioning to actual submissions, and that the information will accordingly be able to be used in a more structured way that better lends itself to automation. The tools used to generate such structured information will be largely those considered in the first of the above points, as tools able to be used for self-evaluation of software. As mentioned above, the actual Submission phase is to be entered in to only following successful assignment of willing reviewers (notwithstanding potential alternative paths, exemplified by current path from rOpenSci review to direct JOSS submission). Proposal Progression from pre-submission to submission be automated as for JOSS. A checklist be automatically generated as part of the opening issue, yet more reflecting current rOpenSci practices of affirming compliant aspects of a submission, rather than JOSS practices of affirming ultimate reviewer judgements. 8.5 Initial Screening The development and provision of automated tools for initial software assessment will enable considerably more structured information to be provided in direct (automated) response to the opening of a submission issue that with the current rOpenSci system. The ready provision of such structured information will aid all of the preceding steps, and will also greatly ease the burden of initial screening of submissions. Software will already have been ascertained to be within scope, willing reviewers will already have been assigned, and an extensive report will have been automatically generated summarising a variety of aspects of software structure, function, and other aspects pertinent to review. The primary purpose of the initial screening step will accordingly be for editors to judge whether or not the totality of submitted data suffices for the review process to officially start. An additional purpose could be the assignment of due dates for submission of reviews. JOSS imposes a generic review period of two weeks, whereas rOpenSci provides opportunity to discuss appropriate due dates with reviewers. Proposal Initial screening involve the two tasks of editors agreeing on submission dates for reviews, and officially approving a submission. The agreement of submission dates be integrated within the official submission issue, rather than the pre-submission issue, so that explicit information on review dates remains within the review issue itself. Submission dates be negotiated around an initial suggested duration of one month. An automated command be implemented for the review process to be “officially” started, which will announce the agreed-upon dates and provide any extra information for reviewers. Note that JOSS currently implements @whedon start review to transition a pre-submission to a full submission. The above suggestions effectively translate to breaking this into the two commands of start submission to transition to a full submission and start review to commence the actual review process once approved by editors. 8.6 Review Process The review processes of rOpenSci and JOSS are qualitatively different, with JOSS submissions guided by extensive automation, and so being strongly determined by their checklist, whereas rOpenSci reviews are commenced only after authors complete the checklist (or otherwise explain any anomalies). Reviewers of submissions to rOpenSci are solicited privately, and privately informed both to read the Guide for Reviewers chapter in the Development, Maintenance, and Peer Review Guide, and that their review must be submitted with the Review Template. This template serves the same purpose as the automatically-generated JOSS template, but is to be pasted by authors themselves into their own comments in the review issue, whereas the JOSS checklist is to be filled out by reviewers editing the opening comment of the review issue. In short, the rOpenSci checklist is an official starting point, with reviews submitted at the end with the help of a template; the JOSS checklist is an official endpoint, empty at first and progressively completed by each reviewer as they progress through the review process. We envision a system primarily derived from rOpenSci’s current system, with reviews completed through the use of a template and pasted as comments at the bottom of a github issue. This approach will face one immediate difficulty in that templates will likely differ through software being described by different combinations of the categories described above. It may suffice to combine a generic “master” template with category-specific items to be appended according to the description of submitted software within our list of categories, although it is important to note that this may exclude review criteria reflecting unique combinations of categories (for example, a checklist item appropriate for the visualisation of results from ML algorithms). The preceding consideration exemplifies the extent to which processes developed and employed to review statistical software are likely to be strongly influenced by the kinds of automated tools we develop, both for automated and self assessment along with associated reporting systems, as well as potentially for more comprehensive assessments and reporting systems or standards not otherwise amenable to automation. In the current initial phase of this project prior to the concrete development of any of these kind of tools, the present considerations of the review process are accordingly and necessarily generic in nature. We anticipate this current sub-section becoming particularly more detailed as the project progress and as we develop project-specific tools for software assessment. 8.6.1 Review Templates As described above, the JOSS checklist is pre-generated with the opening of each review issue, whereas the rOpenSci template is to be completed and pasted in to the issue by reviewers. The two templates are nevertheless broadly similar, both including the following checklist items: The reviewer has no conflict of interests Documentation The software has: A clear statement of need Installation instructions Function documentation Examples Community guidelines for how to contribute Functionality The software should: Install as documented Meet its own functional claims Meet its own performance claims Have automated tests (considered by JOSS as part of “Documentation”) In addition, JOSS requires reviewers: To agree to abide by a reviewer code of conduct To confirm that the source code is in the nominated repository To confirm that the software has an appropriate license. To confirm that the submitting author has made major contributions, and that the provided list of authors seems appropriate and complete. rOpenSci insists in turn on the two additional aspects, that software Has a vignette demonstrating major functionality; and Conforms to the rOpenSci packaging guidelines Perhaps the most influential difference between the two systems is that the rOpenSci template concludes with the following lines: --- ### Review Comments The section break and sub-section heading act in combination as a prompt for reviewers to add their own discursive comments, whereas the JOSS template has no such field. Accordingly, the majority1 of JOSS reviews merely consist of a completed checklist, whereas all rOpenSci reviews are extensively discursive, with reviewers frequently offering very extensive comments and analyses of submitted code. These differences may plausibly be interpreted to reflect general differences in the cultural practices of the two review systems, with rOpenSci having particularly nurtured the cultural practice of extensively discursive reviews, notably through suggesting that prior reviews ought be perused for good guidelines on review practices. We intend to continue to foster and encourage such cultural practices, while at the same time aiming to develop a system for more structured yet discursive input, in order both to provide more focussed software reviews, and to lessen the burden on reviewers. We anticipate commencing the development of such structure in subsequent iterations of the present document. 8.6.2 Category-Specific Aspects of Reviews We defer consideration of category-specific aspects of review until we have concluded a first round of consultation on the preceding categorical definitions. 8.6.3 Reviewer Recommendations Both rOpenSci and JOSS currently work with a binary recommendation scheme of rejection or acceptance. In both cases, rejection is primarily decided in response to a pre-preview (JOSS) or pre-submission enquiry (rOpenSci), and usually for the reason of being out-of-scope (in rOpenSci’s case because software does not fit within the defined categories; and in JOSS’s case because the software does not have a specific research focus). Having obtained approval to proceed from pre-review to full review, both systems generally work with package authors to strive for ultimate acceptance. Rejections during this phase generally only happen when authors stall or abandon ongoing or requested package development. As long as authors continue to be engaged, reviews very generally proceed until a submission is accepted. Proposal While a variety of potential outcomes of the review process are considered immediately below, reviewers will only be requested to ultimately check a single box indicating their approval for software to be accepted. An approved submission may then receive a variety of labels in response to binary acceptance, as described below. Proposal We adopt the current rOpenSci approach of having reviews based on a pre-defined template to be completed by reviewers and pasted as the latest comment in a review issue, rather than the JOSS model of having reviewers edit the initial, opening comment of a review issue. We adopt the current rOpenSci approach of having reviewers testify the time spent on their review. We either then: Do not provide any information on typical times devoted by other reviewers; or Provide summary information including estimates of variation and proviso that such information is only intended to avoid reviewers otherwise feeling obliged to devote unnecessarily long times to reviews. We adopt and adapt the general review templates currently used by rOpenSci and JOSS, extending both in order to provides as much structured discursive feedback as possible. We develop at least examples of category-specific template items to be added to the general review template. 8.7 Acceptance / Scoring / Badging Software being recommended for acceptance by reviews need not be reflected in a simple “accepted” label. Particularly in the early stages of our system for peer-reviewing statistical software, we may have some kind of checklist from which we require authors to ultimately comply with some recommended limited number of items, yet not all. It may then be worthwhile have a review outcome that flags this compliance level, and indicates that software will be expected to retain compliance as our system develops. Another example may be outcomes which consider the kinds of life cycle models considered above, in which context it may be useful to have an outcome that labels software as having passed initial or primary review, yet which will still be subject to subsequent review some agreed-upon time later. Such systems of re-assessment will nevertheless not necessarily be (equally) applicable to all submissions, and so such “progressive labelling” will likely only ever be optional, and applicable where appropriate. Proposal We implement a recommendation system which explicitly flags the version of our system’s standards with which reviewed software complies. 8.8 Post-acceptance Dissemination, Publication, etc. 8.9 Ongoing Maintenance 8.10 Structured Review beyond Acceptance That claim has not been substantiated.↩︎ "],
["python.html", "A Notes on Scope and the Python Statistical Ecosystem A.1 Analysis of statistical software keywords A.2 Bibliography", " A Notes on Scope and the Python Statistical Ecosystem Two factors may be usefully noted in this regard: The potential number of python packages for statistical analyses is likely to be relatively more restricted than relative numbers of R packages. Taking as indicative presentations at the previous three Joint Statistical Meetings (JSMs; 2018-2020), no python packages were referred to in any abstract, while 32 R packages were presented, along with two meta-platforms for R packages. Presentations at the Symposium of Data Science and Statistics (SDSS) for 2018-19 similarly including numerous presentations of R packages, along with presentation of three python packages. It may accordingly be expected that potential expansion to include python packages will demand relatively very little time or effort compared with that devoted to R packages as the primary software scope. In spite of the above, the community of python users is enormously greater, reflected in the currently 232,161 packages compared with 15,576 packages on CRAN, or over 14 times as many python packages. Similarly, 41.7% of all respondents to the 2019 stackoverflow developer survey nominated python as their most popular language, compared with only 5.8% who nominated R. The relative importance of python is powerfully reflected in temporal trends from the stackoverflow developer survey from the previous three years, with results shown in the following graphic. Python is not only more used and more loved than R, but both statistics for python have consistently grown at a faster rate over the past three years as have equivalent statistics for R. Both languages nevertheless have relative well-defined standards for software packaging, python via the Python Package Index (pypi), and R via CRAN. In contrast to CRAN, which runs its own checks on all packages on a daily basis, there are no automatic checks for pypi packages, and almost any form of package that minimally conforms to the standards may be submitted. This much lower effective barrier to entry likely partially contributes to the far greater numbers of pypi (232,161) than CRAN (15,576) packages. A.1 Analysis of statistical software keywords The JOSS conducts its own peer review process, and publishes textual descriptions of accepted software. Each piece of software then has its own web page on the journal’s site, on which the text is presented as a compiled .pdf-format document, along with links to the open review, as well as to the software repository. The published document must be included within the software repository in a file named paper.md, which enables automatic extraction and analysis of these text descriptions of software. Rather than attempt a comprehensive, and unavoidably subjective, categorization of software, these textual descriptions were used to identify key words or phrases (hereafter, “keywords”) which encapsulated the purpose, function, or other general descriptive elements of each piece of software. Each paper generally yielded multiple keywords. Extracting these from all papers judged to be potentially in scope allowed for the construction of a network of topics, in which the nodes were the key words and phrases, and the connections between any pair of nodes reflected the number of times those two keywords co-occured across all papers. We extracted all papers accepted and published by JOSS (217 at the time of writing in early 2020), and manually determined which of these were broadly statistical, reducing the total to 92. We then read through the contents of each of these, and recorded as many keywords as possible for each paper. The resultant network is shown in the following interactive graphic, in which nodes are scaled by numbers of occurrences, and edges by numbers of co-occurrences. (Or click here for full-screen version with link to code.) Such a network visualization enables immediate identification of more and less central concepts including, in our case, several that we may not otherwise have conceived of as having been potentially in scope. We then used this network to define our set of key “in scope” concepts. This figure also reveals that many of these keywords are somewhat “lower level” than the kinds of concepts we might otherwise have used to define scoping categories. For example, keywords such as “likelihood” or “probability” are not likely to be useful in defining actual categories of statistical software, yet they turned out to lie at the centres of relatively well-defined groups of related keywords. We also examined the forms of both input and output data for each of the 92 pieces of software described in these JOSS papers, and constructed an additional graph directionally relating these different data formats. A.2 Bibliography "]
]
